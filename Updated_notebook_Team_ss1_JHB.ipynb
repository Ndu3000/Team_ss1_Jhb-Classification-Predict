{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Updated notebook Team_ss1_JHB.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/blob/master/Updated_notebook_Team_ss1_JHB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nzbv64tkF16w"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Context\" data-toc-modified-id=\"Context-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Context</a></span></li><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Problem Statement</a></span></li><li><span><a href=\"#Data-sets\" data-toc-modified-id=\"Data-sets-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Data sets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#Variable-definitions\" data-toc-modified-id=\"Variable-definitions-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Variable definitions</a></span></li><li><span><a href=\"#The-data-input-files-we-have-used-for-our-model-are:\" data-toc-modified-id=\"The-data-input-files-we-have-used-for-our-model-are:-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>The data input files we have used for our model are:</a></span></li></ul></li></ul></li><li><span><a href=\"#Initial-Data-Exploration\" data-toc-modified--2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Initial Data Exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-packages-and-load-data-files\" data-toc-modified-id=\"Import-packages-and-load-data-files-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Import packages and load data files</a></span></li><li><span><a href=\"#Missing-Values\" data-toc-modified-id=\"Missing-Values-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Missing Values</a></span></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Insights\" data-toc-modified-id=\"Insights-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Insights</a></span></li></ul></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initial-model-evaluation\" data-toc-modified-id=\"Initial-model-evaluation-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Initial model evaluation</a></span></li><li><span><a href=\"#Further-Feature-Extraction\" data-toc-modified-id=\"Further-Feature-Extraction-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Further Feature Extraction</a></span></li></ul></li><li><span><a href=\"#Predictive-Modelling\" data-toc-modified-id=\"Predictive-Modelling-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Predictive Modelling</a></span><ul class=\"toc-item\"><li><span><a href=\"#An-Overview-of-learners\" data-toc-modified-id=\"An-Overview-of-learners-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>An Overview of learners</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"#K-Nearest-Neighbours\" data-toc-modified-id=\"K-Nearest-Neighbours-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>K-Nearest Neighbours</a></span></li><li><span><a href=\"#Support-Vector-Machines\" data-toc-modified-id=\"Support-Vector-Machines-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Support Vector Machines</a></span></li><li><span><a href=\"#Naïve-Bayes\" data-toc-modified-id=\"Naïve-Bayes-5.1.4\"><span class=\"toc-item-num\">5.1.4&nbsp;&nbsp;</span>Naïve Bayes</a></span></li><li><span><a href=\"#Decision-Tree\" data-toc-modified-id=\"Decision-Tree-5.1.5\"><span class=\"toc-item-num\">5.1.5&nbsp;&nbsp;</span>Decision Tree</a></span></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-5.1.6\"><span class=\"toc-item-num\">5.1.6&nbsp;&nbsp;</span>Random Forest</a></span></li></ul></li><li><span><a href=\"#An-Overview-of-the-features\" data-toc-modified-id=\"An-Overview-of-the-features-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>An Overview of the features</a></span></li><li><span><a href=\"#An-explanation-of-Pipelines\" data-toc-modified-id=\"An-explanation-of-Pipelines-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>An explanation of Pipelines</a></span></li><li><span><a href=\"#Build-a-pipeline-to-vectorize-the-data,-then-train-and-fit-a-model\" data-toc-modified-id=\"Build-a-pipeline-to-vectorize-the-data,-then-train-and-fit-a-model-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Build a pipeline to vectorize the data, then train and fit a model</a></span></li><li><span><a href=\"#Run-predictions-and-analyze-the-results\" data-toc-modified-id=\"Run-predictions-and-analyze-the-results-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Run predictions and analyze the results</a></span></li></ul></li><li><span><a href=\"#Feature-Selection-and-Model-Selection\" data-toc-modified-id=\"Feature-Selection-and-Model-Selection-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Feature Selection and Model Selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameter-tuning\" data-toc-modified-id=\"Hyperparameter-tuning-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Hyperparameter tuning</a></span></li><li><span><a href=\"#Model-Evaluation\" data-toc-modified-id=\"Model-Evaluation-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Model Evaluation</a></span></li></ul></li><li><span><a href=\"#Summary-of-Conclusions\" data-toc-modified-id=\"Summary-of-Conclusions-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Summary of Conclusions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Data Exploration</a></span></li><li><span><a href=\"#Exploratory-data-analysis\" data-toc-modified-id=\"Exploratory-data-analysis-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Exploratory data analysis</a></span></li><li><span><a href=\"#Predictive-modelling\" data-toc-modified-id=\"Predictive-modelling-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Predictive modelling</a></span></li><li><span><a href=\"#Feature-selection\" data-toc-modified-id=\"Feature-selection-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Feature selection</a></span></li><li><span><a href=\"#Recommendations\" data-toc-modified-id=\"Recommendations-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Recommendations</a></span></li><li><span><a href=\"#Key-takeaways\" data-toc-modified-id=\"Key-takeaways-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>Key takeaways</a></span></li></ul></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i7XMGIMFkyOb"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rJ0GgUJUObLr"
      },
      "source": [
        "# 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "heading_collapsed": true,
        "id": "pP7AYi2lGM09"
      },
      "source": [
        "## 1.1 Context\n",
        "\n",
        "Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cLFB14bwTMrk"
      },
      "source": [
        "## 1.2 Problem Statement\n",
        "\n",
        "With this context, we are being challenged with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
        "\n",
        "Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.\n",
        "\n",
        "Our quest is to provide valuable sentiment analysis information with actionable insights that can be used for an entity that  could like its brand to be viewed as \"eco-friendly\" from the perspective of all its stakeeholders e.g. clients, employeers, investors etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5rcVGaPfAA0I"
      },
      "source": [
        "## 1.3 Data Description\n",
        "\n",
        "### 1.3.1 Data\n",
        "\n",
        "The collection of this data which we use as our data source was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo. The dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. In total, 43943 tweets were collected. Each tweet is labelled as one of the following classes in the table below:\n",
        "\n",
        "<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2205222%2F8e4d65f2029797e0462b52022451829c%2Fdata.PNG?generation=1590752860255531&amp;alt=media\" alt=\"\" title=\"\">\n",
        "\n",
        "### 1.3.2 Variable definitions\n",
        "\n",
        "*   sentiment: Sentiment of tweet\n",
        "\n",
        "*   message: Tweet body\n",
        "\n",
        "*   tweetid: Twitter unique id\n",
        "\n",
        "\n",
        "### 1.3.3 The data input files we have used for our model are:\n",
        "\n",
        "*   Train.csv (is the dataset that we will use to train to our model) as denoted by the \"train_data\" dataframe variable in our code.\n",
        "*   Test.csv (is the dataset on which we will apply to our model to) as denoted by the \"test_data\" dataframe variable in our code.\n",
        "*   SampleSubmission.csv (is an example of what our submission file will look like. The order of the rows is not so relevant, but the names of the tweetid's must be correct.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xms0kqxDGM0-"
      },
      "source": [
        "# 2. Download, import packages and load data files "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gU9fdfaGGM0_"
      },
      "source": [
        "###### Task: Download and install external libraries/packages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vQSytCTMwJnN"
      },
      "source": [
        "### 2.1 Install relevant libraries\n",
        "\n",
        "We start off by importing relevant packages and loading the data.\n",
        "\n",
        "`Comet:`for viewing different versions of your machinle learning model results - `pip install comet_ml--3.1.11`\n",
        "\n",
        "`Enoji:` for removing emojis - `pip install emoji--0.5.4`\n",
        "\n",
        "`Wordcloud:`for creating a word cloud infographic to visualise frequent words used in text - `pip install wordcloud --1.7.0`\n",
        "\n",
        "`Textblob:`for simple, Pythonic text processing. Sentiment analysis, part-of-speech tagging, noun phrase parsing, and more - `pip install textblob--0.15.3` \n",
        "\n",
        "`Natural Language Toolkit (NLTK):`for common natural language processing (NLP) tasks and more - `pip install nltk--3.4.5`\n",
        "\n",
        "`Spacy:`for industrial strength language processing (NLP) tasks and more - `pip install spacy--2.2.4`\n",
        "\n",
        "`Tensorflow:`or for high performance machine learning numerical computation - `pip install tensorflow--2.2.0`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r-6p508-hg7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "602e5506-a0b1-4c82-e22c-2e32d4e0a5a0"
      },
      "source": [
        "# Install relevant external libraries\n",
        "!pip install comet_ml\n",
        "!pip install emoji\n",
        "!pip install wordcloud\n",
        "!pip install textblob\n",
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!pip install tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting comet_ml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ac/d8/99564b9b0d68a8acce733a5b053497a4a51e3ffdc54e4b117808bd6c4c94/comet_ml-3.1.12-py2.py3-none-any.whl (214kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 22.5MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 7.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 40kB 9.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 51kB 7.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 61kB 7.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 71kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 81kB 9.0MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 92kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 102kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 112kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 122kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 133kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 143kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 153kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 163kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 174kB 8.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 184kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 194kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 204kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 215kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.6.0)\n",
            "Collecting websocket-client>=0.55.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\r\u001b[K     |█▋                              | 10kB 28.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 20kB 37.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 45.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 40kB 43.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 51kB 18.2MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 61kB 20.8MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 71kB 20.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 81kB 21.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 92kB 21.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 102kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 112kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 122kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 133kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 143kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 153kB 18.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 163kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 174kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 184kB 18.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 194kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 18.6MB/s \n",
            "\u001b[?25hCollecting wurlitzer>=1.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/24/5e/f3bd8443bfdf96d2f5d10097d301076a9eb55637b7864e52d2d1a4d8c72a/wurlitzer-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from comet_ml) (1.12.0)\n",
            "Collecting comet-git-pure>=0.19.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/7a/483413046e48908986a0f9a1d8a917e1da46ae58e6ba16b2ac71b3adf8d7/comet_git_pure-0.19.16-py3-none-any.whl (409kB)\n",
            "\r\u001b[K     |▉                               | 10kB 24.3MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20kB 32.3MB/s eta 0:00:01\r\u001b[K     |██▍                             | 30kB 34.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 40kB 37.5MB/s eta 0:00:01\r\u001b[K     |████                            | 51kB 33.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 61kB 36.2MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 71kB 23.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 81kB 25.8MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 92kB 24.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 102kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 112kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 122kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 133kB 22.5MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 143kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 153kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 163kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 174kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 184kB 22.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 194kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 204kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 215kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 225kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 235kB 22.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 245kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 256kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 266kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 276kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 286kB 22.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 296kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 307kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 317kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 327kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 337kB 22.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 348kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 358kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 368kB 22.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 378kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 389kB 22.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 399kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 409kB 22.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 419kB 22.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.23.0)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (7.352.0)\n",
            "Collecting netifaces>=0.10.7\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/9b/c4c7eb09189548d45939a3d3a6b3d53979c67d124459b27a094c365c347f/netifaces-0.10.9-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting everett[ini]>=1.0.1; python_version >= \"3.0\"\n",
            "  Downloading https://files.pythonhosted.org/packages/12/34/de70a3d913411e40ce84966f085b5da0c6df741e28c86721114dd290aaa0/everett-1.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3>=1.24.1 in /usr/local/lib/python3.6/dist-packages (from comet-git-pure>=0.19.11->comet_ml) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from comet-git-pure>=0.19.11->comet_ml) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (2.9)\n",
            "Collecting configobj; extra == \"ini\"\n",
            "  Downloading https://files.pythonhosted.org/packages/64/61/079eb60459c44929e684fa7d9e2fdca403f67d64dd9dbac27296be2e0fab/configobj-5.0.6.tar.gz\n",
            "Building wheels for collected packages: configobj\n",
            "  Building wheel for configobj (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configobj: filename=configobj-5.0.6-cp36-none-any.whl size=34546 sha256=bdf3394b266a6d94e6310df348139dc686f1d86f8ba8a32be9abd174b74cdc83\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/e4/16/4981ca97c2d65106b49861e0b35e2660695be7219a2d351ee0\n",
            "Successfully built configobj\n",
            "Installing collected packages: websocket-client, wurlitzer, comet-git-pure, netifaces, configobj, everett, comet-ml\n",
            "Successfully installed comet-git-pure-0.19.16 comet-ml-3.1.12 configobj-5.0.6 everett-1.0.2 netifaces-0.10.9 websocket-client-0.57.0 wurlitzer-2.0.0\n",
            "Collecting emoji\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=192d7c0906623a55f6d018878e355c37a8e91eeb4f7ab5817f8ce4ccc536de69\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-0.5.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ba126e2ed597>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install comet_ml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install emoji'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install wordcloud'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install textblob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install nltk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     with temporary_clearer(), _display_stdin_widget(\n\u001b[0;32m--> 181\u001b[0;31m         delay_millis=500) as update_stdin_widget:\n\u001b[0m\u001b[1;32m    182\u001b[0m       \u001b[0;31m# TODO(b/115531839): Ensure that subprocesses are terminated upon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# interrupt.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0mshell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0mdisplay_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_display_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'delayMillis'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdelay_millis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdisplay_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mecho_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_echo_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: CustomError: Timed out waiting for output iframe load."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9F3gLo_Kke0b",
        "colab": {}
      },
      "source": [
        "# Import packages needed to solve the problem\n",
        "\n",
        "# Data analysis and wrangling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# General natural language and text processing tools\n",
        "import re\n",
        "import string\n",
        "import emoji\n",
        "import gensim\n",
        "import spacy\n",
        "import sklearn.feature_extraction.text\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import defaultdict\n",
        "from collections import  Counter\n",
        "\n",
        "# Enhanced natural language and text processing tools\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# Processing data to prior to trainig and fitting to model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Stemming/lemmatizing/vectorizing\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Data visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "# Set plot style for data visualisation\n",
        "sns.set()\n",
        "# for improved aesthetics\n",
        "plt.style.use('ggplot')    \n",
        "\n",
        "# Importing wordcloud for plotting word clouds and \n",
        "from wordcloud import WordCloud\n",
        "# textwrap for wrapping longer text\n",
        "from textwrap import wrap\n",
        "\n",
        "# Machine Learning model versioning\n",
        "from comet_ml import Experiment\n",
        "\n",
        "# Machine learning model evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Ignore warnings because they are annoying\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vYnfg2-A9TII"
      },
      "source": [
        "### 2.2 Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vl9GIZCoGM1F",
        "colab": {}
      },
      "source": [
        "# Load train data\n",
        "train_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Y7rqWgIlPXq",
        "colab": {}
      },
      "source": [
        "# Load test data\n",
        "test_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uwmD9v11lPXw",
        "colab": {}
      },
      "source": [
        "# Load sample data\n",
        "sample_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/sample_submission.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rSbjmyfQGM1i"
      },
      "source": [
        "###### **Task: View sample of train and test data**\n",
        "Simply displaying the train and test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4PMfA1FOGM1i",
        "colab": {}
      },
      "source": [
        "# Print train_data\n",
        "print('Train data rows and columns:')\n",
        "train_data.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ddd8_l59GM1o",
        "colab": {}
      },
      "source": [
        "# Print test_data\n",
        "print('Test data rows and columns:')\n",
        "test_data.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UBXHM2u2GM1P"
      },
      "source": [
        "###### **Task: Display the shape of train and test data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsnTLYIVGM1Q",
        "colab": {}
      },
      "source": [
        "print('Train data rows and columns:', train_data.shape)\n",
        "print('Test data rows and columns:', test_data.shape)\n",
        "print('Sample data rows and columns:', sample_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YTkU_GH9GM1U"
      },
      "source": [
        "###### **Task: Investigate the label column.**\n",
        "After loading and viewing the shape (i.e. how many rows and columns exist in our dataset) of our data we now take a cursory view of the target variable that we ultimately need to predict. In our initial data exploration we will take a brief look at the proportionality of data to see whether it is balanced or not. In the real world, text data is rarely balanced so we expect to see imbalanced proportionality between the four class labels associated with the target variable that we aim to predict. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b5f_ZH8KlPYW",
        "colab": {}
      },
      "source": [
        "# View all the class labels for the target variable\n",
        "train_data['sentiment'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SnRzjZvylPYf",
        "colab": {}
      },
      "source": [
        "# View the breakdown of the different class labels\n",
        "train_data['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sJldCV1e9caz"
      },
      "source": [
        "# 3. Initial Data Exporation Analysis\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9L9g74L1GM2j"
      },
      "source": [
        "###### **Task: Check the proportionality of the class labels to see if data is indeed imbalanced.**\n",
        "\n",
        "Check class label proportion for:\n",
        "\n",
        "* **Anti:** the tweet does not believe in man-made climate change (**class =  -1**)\n",
        "\n",
        "* **Neutral:** the tweet neither supports nor refutes the belief of man-mad climate change (**class =  0**)\n",
        "\n",
        "* **Pro:** the tweet supports the belief of man-made climate change (**class = 1**)\n",
        "\n",
        "* **News:** the tweet links to factual news about climate change (**class = 2**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H1KJOfkSGM2k",
        "colab": {}
      },
      "source": [
        "# Calculate class label proportions\n",
        "anti_class_proportion = len(train_data.loc[train_data['sentiment']== -1]) / len(train_data)\n",
        "neut_class_proportion = len(train_data.loc[train_data['sentiment']==0]) / len(train_data)\n",
        "pro_class_proportion = len(train_data.loc[train_data['sentiment']==1]) / len(train_data)\n",
        "news_class_proportion = len(train_data.loc[train_data['sentiment']==2]) / len(train_data)\n",
        "\n",
        "# View class label proportions\n",
        "\n",
        "print(\"Anti class proportion: -1 =\", anti_class_proportion)\n",
        "print(\"Neutral class proportion: 0 =\",neut_class_proportion)\n",
        "print(\"Pro class proportion: 1 =\",pro_class_proportion)\n",
        "print(\"News class proportion: 2 =\",news_class_proportion)\n",
        "\n",
        "##ROUND THE PROPORTIONS TO 2 DECIMALS AND LABEL THEM ##BILE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "khOQ1I3vlPZA"
      },
      "source": [
        "###### **Task: Check that the sum of all class label proportions are equal to 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YwF0t6FPlPZB",
        "colab": {}
      },
      "source": [
        "# Calculate and confirm that the above proportions are in fact equal to 1 (or 100%)\n",
        "total_class = anti_class_proportion + neut_class_proportion + pro_class_proportion + news_class_proportion\n",
        "total_class == 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uVoFPV3HzZs_",
        "colab": {}
      },
      "source": [
        "# Set figure size for ditribution of class imbalance\n",
        "fig, ax = plt.subplots(figsize = (10, 5))\n",
        "\n",
        "# Create ditribution bar graph \n",
        "graph = sns.countplot(x = 'sentiment', data = train_data)\n",
        "\n",
        "# Give title and plot\n",
        "plt.title('Distribution of Classification Groups')\n",
        "plt.xlabel('Sentiment class labels')\n",
        "plt.ylabel('Number of tweets')\n",
        "plt.show(graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zNzczVZKzkiW"
      },
      "source": [
        "## summary of findings ## NONDU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zZCEZAppARVp",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# To view proportions of the class labels its best practice to us pie charts\n",
        "# Where the slices will be ordered and plotted counter-clockwise:\n",
        "labels = 'Anti', 'Neutral', 'Pro', 'News'\n",
        "sizes = [anti_class_proportion, neut_class_proportion, pro_class_proportion, news_class_proportion]\n",
        "explode = (0, 0, 0.1, 0)  # Only \"explode\" the 3rd slice (i.e. 'Anti')\n",
        "\n",
        "# Create pie chart with the above labels and calculated class proportions as inputs\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
        "        shadow=True, startangle=90)\n",
        "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nddkyb6uBOSn"
      },
      "source": [
        "## NDU TO ADD COMMENTS ON CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7FSXyjWv6U_-"
      },
      "source": [
        "###3.1 Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iDq04RN5-6wG"
      },
      "source": [
        "#### Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UOEqzKnNGM2Z"
      },
      "source": [
        "Missing values are a common attribute in datasets and for a number of different reasons. In this part of the notebook we will do a minimal assessment of missing values. It is important however that we understand missingness from both the perspective of the train_data and test_data. As such we want to see which columns have missing data in both the train and test datasets as well as the proportion of missingness in each of those columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rxm0l8PxGM2a"
      },
      "source": [
        "###### **Task: Check for missing values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XwADQdKTGM2a",
        "colab": {}
      },
      "source": [
        "# View missing values for train data\n",
        "train_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hu5C-7ehGM2f"
      },
      "source": [
        "###### **Task: Check for whitespace strings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cxVU6lryGM2g",
        "colab": {}
      },
      "source": [
        "blanks = []  # start with an empty list\n",
        "\n",
        "for i,lb,msg,tid in train_data.itertuples():  # iterate over the DataFrame\n",
        "    if type(msg)==str:            # avoid NaN values\n",
        "        if msg.isspace():         # test 'review' for whitespace\n",
        "            blanks.append(i)     # add matching index numbers to the list\n",
        "        \n",
        "print(len(blanks), 'blanks: ', blanks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f9uEPM-PlPZa"
      },
      "source": [
        "###### **task: Check for possible duplicate tweets/retweets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lwVFt0PilPZb"
      },
      "source": [
        "<img data-attachment-id=\"1700\" data-permalink=\"https://tomraftery.com/2011/06/01/my-twitter-magic-number-is-16-whats-yours/screen-shot-2011-06-01-at-20-25-36/\" data-orig-file=\"https://i2.wp.com/tomraftery.com/wp-content/uploads/2011/06/screen-shot-2011-06-01-at-20-25-36.png?fit=563%2C271&amp;ssl=1\" data-orig-size=\"563,271\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}\" data-image-title=\"Twitter post\" data-image-description=\"<p>Twitter post staying under my Twitter magic number</p>\n",
        "\" data-medium-file=\"https://i2.wp.com/tomraftery.com/wp-content/uploads/2011/06/screen-shot-2011-06-01-at-20-25-36.png?fit=300%2C144&amp;ssl=1\" data-large-file=\"https://i2.wp.com/tomraftery.com/wp-content/uploads/2011/06/screen-shot-2011-06-01-at-20-25-36.png?fit=563%2C271&amp;ssl=1\" class=\"size-full wp-image-1700 jetpack-lazy-image jetpack-lazy-image--handled\" title=\"Twitter post\" src=\"https://i0.wp.com/www.enterpriseirregulars.com/wp-content/uploads/2011/06/a06675977df79f3e834fdf758225008f1.png?resize=563%2C271&amp;ssl=1\" alt=\"Twitter post\" width=\"563\" height=\"271\" data-recalc-dims=\"1\" data-lazy-loaded=\"1\">\n",
        "\n",
        "<p>Twitter is a superb medium for getting a message out.</p>\n",
        "\n",
        "<p>And it’s RT (Retweet) convention means that tweets can go viral very quickly. So this may give us an indication of virality and sentiment for certain classes of tweets as there are a number of RT's visible in our data. But how many are there? and for which sentiment classes?</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pawf4ezIjp8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check for duplicate messages/tweets in the train data\n",
        "dups_train = train_data['message']\n",
        "dups_train = train_data[dups_train.isin(dups_train[dups_train.duplicated()])].sort_values(\"message\")\n",
        "# Check for duplicate tweet ID's in the train data to validate that each message is unique\n",
        "dups_tweet_tr = train_data['tweetid']\n",
        "train_data[dups_tweet_tr.isin(dups_tweet_tr[dups_tweet_tr.duplicated()])].sort_values(\"message\")\n",
        "# Check for duplicate messages/tweets in the test data\n",
        "dups_test = test_data['message']\n",
        "dups_test = test_data[dups_test.isin(dups_test[dups_test.duplicated()])].sort_values(\"message\")\n",
        "# Check for duplicate tweet ID's in the test data to validate that each message is unique\n",
        "dups_tweet_te = train_data['tweetid']\n",
        "train_data[dups_tweet_te.isin(dups_tweet_te[dups_tweet_te.duplicated()])].sort_values(\"message\")\n",
        "\n",
        "print('Duplicate tweet messages in train data rows and columns:',dups_train.shape)\n",
        "print('Duplicate tweet messages in test data rows and columns:',dups_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zuFJIDzalPZs",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "#Confirm whether sample of duplicates in train data is generally comprised of retweets\n",
        "dups_train.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AqTM3d66lPaC",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "#Confirm whether sample of duplicates in test data is generally comprised of retweets\n",
        "dups_test.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6w134PL5lPaQ",
        "colab": {}
      },
      "source": [
        "# View the differences in proportions of duplicates across train and test datasets\n",
        "dups_train_prop = len(dups_train)/len(train_data['message'])\n",
        "dups_test_prop = len(dups_test)/len(test_data['message'])\n",
        "print('Train data proportion of duplicates/RTs:',round((dups_train_prop),2))\n",
        "print('Test data proportion of duplicates/RTs:',round((dups_test_prop),2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zr_MrlKZlPaV"
      },
      "source": [
        "12 per cent of our train data contains duplicates and 11% of our test data contains duplicates. The propotion of duplicate messages across both data sets is similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Af_xQ6CHObNp"
      },
      "source": [
        "## SUMMARISE INITIAL FINDING  \n",
        "\n",
        "**Exploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations\")**\n",
        "\n",
        "****After pefroming  EDA on the given data the following observations were made:\n",
        " 1 (pro)   8530\n",
        " 2 (News)  3640\n",
        " 0 (Neutral)  2353\n",
        "-1 (Anti)   1296****\n",
        "\n",
        "***this suggests that over 60% of the tweets are in support of man-made climate change while 20% were neutral, we can assume(hypothesis) that a positive change can be made as there are more pro sentiments than anti and a 50% chance of influencing the neutral sentiments to pro sentiments***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i0LW5Pq2D7fI"
      },
      "source": [
        "## EXPLAIN ALL THE STEPS OF CODE # NDU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UXuEt0qblPaY"
      },
      "source": [
        "######In the next several tasks we will take a first iteration taking a very manual text preprocessing approach to text cleaning which works but is imperfect in order to show that text cleaning in NLP can be an iterative process which is defined by the context of data you need for the machine learning problem you are solving for. Here we show a step by step approach of some of the text cleaning tools available to us. \n",
        "\n",
        "Further down in the notebook we take a more simplified approach once we understand the steps involved and how each step modifies our data so that it is ready for some more data exploration and is useable for our prediction model for this particular problem which relates to multiclass classification for those who believe and don't believe that climate change exists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nKUk8-LnlPaf"
      },
      "source": [
        "###### **task: review sample of uncleaned tweets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pF_MqYhvlPaf",
        "colab": {}
      },
      "source": [
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OqR7EPr2lPak",
        "colab": {}
      },
      "source": [
        "# try removing non-ASCII strings as a start e.g. byÃ¢â‚¬Â¦\n",
        "train_data['message']=train_data['message'].apply(lambda x: x.split('Ã¢â‚¬Â¦')[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iJdcYJualPao",
        "colab": {}
      },
      "source": [
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "biDGHQ9AlPav",
        "colab": {}
      },
      "source": [
        "# remove URL's\n",
        "train_data['message']=train_data['message'].apply(lambda x: re.sub(r\"\\bhttps://t.co/\\w+\", '', x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YS9ybyoLlPaz",
        "colab": {}
      },
      "source": [
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tw0H_b7glPa6",
        "colab": {}
      },
      "source": [
        "# line breaks\n",
        "train_data['message']=train_data['message'].replace('\\n', ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E9Tq01DolPa_",
        "colab": {}
      },
      "source": [
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1kQtS1m1lPbF",
        "colab": {}
      },
      "source": [
        "# remove numbers\n",
        "train_data['message']=train_data['message'].apply(lambda x: re.sub('\\w*\\d\\w*', ' ', x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZoZ5LCcllPbJ",
        "colab": {}
      },
      "source": [
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U8MUmds8lPbM",
        "colab": {}
      },
      "source": [
        "# remove capital letters and punctuations\n",
        "\n",
        "train_data['message']=train_data['message'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R-EbhW_PlPbQ",
        "colab": {}
      },
      "source": [
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QDwpHRznlPbV",
        "colab": {}
      },
      "source": [
        "# Dictionary of English Contractions\n",
        "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
        "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\", \"it's\": \"it is\" ,\n",
        "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
        "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
        "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
        "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
        "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
        "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
        "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
        "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
        "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
        "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
        "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
        "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
        "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
        "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
        "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
        "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
        "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
        "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
        "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
        "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
        "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
        "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
        "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
        "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
        "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
        "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
        "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
        "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
        "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
        "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
        "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
        "                     \"you've\": \"you have\", \"we’ ve\": \"we have\", \"imvotingbecause\": \"i am voting because\", \"rt\": \"\" }\n",
        "\n",
        "# Regular expression for finding contractions\n",
        "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "# Function for expanding contractions\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "  def replace(match):\n",
        "    return contractions_dict[match.group(0)]\n",
        "  return contractions_re.sub(replace, text)\n",
        "\n",
        "# Expanding Contractions in the reviews\n",
        "train_data['message']=train_data['message'].apply(lambda x:expand_contractions(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p71-AMHTlPbY",
        "colab": {}
      },
      "source": [
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e2oBIxYilPbc",
        "colab": {}
      },
      "source": [
        "train_data['message']=train_data['message'].apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YUVXTliOlPbh",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IRWZvtSLlPbl",
        "colab": {}
      },
      "source": [
        "train_data['message']=train_data['message'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m42Tj20ylPbr",
        "colab": {}
      },
      "source": [
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fetOHgEalPbv",
        "colab": {}
      },
      "source": [
        "# remove punctuations\n",
        "\n",
        "train_data['message']=train_data['message'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hx0ujRAOlPb1",
        "colab": {}
      },
      "source": [
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Om5VFsVKlPb5",
        "colab": {}
      },
      "source": [
        "# remove extra spaces\n",
        "train_data['message']=train_data['message'].apply(lambda x: re.sub(' +',' ',x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JLvQX2rplPb9",
        "colab": {}
      },
      "source": [
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g20omEG9lPcC",
        "colab": {}
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9ZvvFOV9lPcI",
        "colab": {}
      },
      "source": [
        "# Function to remove emojis\n",
        "!pip install emoji\n",
        "import emoji\n",
        "import string\n",
        "\n",
        "def give_emoji_free_text(text):\n",
        "    \n",
        "    '''\n",
        "    Takes in tweet series, removes all emojis, and returns cleaned tweet series.\n",
        "    '''\n",
        "    \n",
        "    allchars = [str for str in text]\n",
        "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
        "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
        "\n",
        "    return clean_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GEMHAPeNlPcM",
        "colab": {}
      },
      "source": [
        "full_text_list = []\n",
        "\n",
        "for index, rows in train_data['message'].iteritems():\n",
        "    rows = give_emoji_free_text(rows) # remove emojis\n",
        "    full_text_list.append(rows)\n",
        "    \n",
        "train_data['message'] = full_text_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TM3G9Oc4lPcQ",
        "colab": {}
      },
      "source": [
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TntwyQJMlPcS",
        "colab": {}
      },
      "source": [
        "# try removing non-ASCII strings as a start e.g. iã¢â‚¬â¦\n",
        "train_data['message']=train_data['message'].apply(lambda x: x.split('iã¢â‚¬â¦')[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E38MD179lPcX",
        "colab": {}
      },
      "source": [
        "for index,text in enumerate(train_data['message'][35:40]):\n",
        "  print('Message %d:\\n'%(index+1),text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "74bGhzJqlPca",
        "colab": {}
      },
      "source": [
        "train_data.to_csv('new train.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZPemIZeDlPcc",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('new train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "meLCmaPFlPce",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DPubkW_-lPch"
      },
      "source": [
        "###### **task: lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gsZqLUOUlPch",
        "colab": {}
      },
      "source": [
        "# Loading model\n",
        "nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KUw2tp1xlPcj",
        "colab": {}
      },
      "source": [
        "# tokenization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ynuZg8PElPco",
        "colab": {}
      },
      "source": [
        "# Lemmatization with stopwords removal\n",
        "train_data['lemmatized']=train_data['lemmatized']=train_data['message'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))\n",
        "train_data['lemmatized']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nYY57wBOlPcr",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "train_data_grouped=train_data[['message','lemmatized']].groupby(by='message').agg(lambda x:' '.join(x))\n",
        "train_data_grouped.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S5EhTbYMlPct",
        "colab": {}
      },
      "source": [
        "#train_data.drop('message', axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vg30L4PilPcw",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "erI5JZZmlPcy",
        "colab": {}
      },
      "source": [
        "train_data['lemmatized'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ytB6qkkqlPc1",
        "colab": {}
      },
      "source": [
        "for index,text in enumerate(train_data['lemmatized'][35:40]):\n",
        "  print('Lemmatized Message %d:\\n'%(index+1),text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IuwZ-qcmlPc3"
      },
      "source": [
        "###### **task: Document Term Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4LHZl5ULlPc4",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Creating Document Term Matrix\n",
        "token = RegexpTokenizer(r'[@a-zA-Z0-9]+')\n",
        "cv=CountVectorizer(analyzer='word',lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
        "text_counts=cv.fit_transform(train_data['lemmatized'])\n",
        "df_dtm = pd.DataFrame(text_counts.toarray(), columns=cv.get_feature_names())\n",
        "df_dtm.index=train_data['lemmatized'].index\n",
        "df_dtm.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D7zWIFrXlPdA"
      },
      "source": [
        "# 4. Further Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m6LSV9GqlPdA",
        "colab": {}
      },
      "source": [
        "# load train data\n",
        "train_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O0dohfbilPdD",
        "colab": {}
      },
      "source": [
        "# load test data\n",
        "test_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/test.csv')\n",
        "print('Test data rows and columns:')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GqDBwDc2lPdF",
        "colab": {}
      },
      "source": [
        "# load sample data\n",
        "sample_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/sample_submission.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iYBK056OlPdK",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "#this is just cool\n",
        "from tqdm import tqdm\n",
        "\n",
        "#visualization\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')   #for optimum aesthetics \n",
        "import seaborn as sns\n",
        "\n",
        "#natural language processing\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gensim\n",
        "import spacy\n",
        "\n",
        "#stemming/lemmatizing/vectorizing\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#processing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#ignore warnings because they are annoying\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from nltk.util import ngrams\n",
        "from collections import defaultdict\n",
        "from collections import  Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "91ue8sWm_BWc"
      },
      "source": [
        "**Task: Visualize current state of data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_iGCdfQflPdR",
        "colab": {}
      },
      "source": [
        "def get_top_tweet_bigrams(corpus, n=None): ##NDU CHANGE \n",
        "    vec = CountVectorizer(ngram_range=(1, 1)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tF7S_xfQJvNl"
      },
      "source": [
        "## COMMENT ON THE DIAGRAM BEFORE THE CLUSTERING. GIVE DETAILED STORY \n",
        "## NDU AND VICKY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZCdWoLGblPdU",
        "colab": {}
      },
      "source": [
        "## NDU CHANGE\n",
        "#Initial minimally cleaned text for top 20 unigram count\n",
        "plt.figure(figsize=(10,5))\n",
        "top_tweet_bigrams=get_top_tweet_bigrams(train_data['message'])[:20]\n",
        "x,y=map(list,zip(*top_tweet_bigrams))\n",
        "sns.barplot(x=y,y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dLHa2DPmlPdW",
        "colab": {}
      },
      "source": [
        "#Initial minimally cleaned text for top 20 bigram count\n",
        "plt.figure(figsize=(10,5))\n",
        "top_tweet_bigrams=get_top_tweet_bigrams(train_data['message'])[:20]\n",
        "x,y=map(list,zip(*top_tweet_bigrams))\n",
        "sns.barplot(x=y,y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MAo_9M_5lPdb",
        "colab": {}
      },
      "source": [
        "#create column for the number of words in tweet\n",
        "train_data['word_count'] = train_data['message'].apply(lambda x: len(x.split()))\n",
        "\n",
        "#split so we can use updated train set with new feature\n",
        "train_data = train_data[:len(train_data)]\n",
        "\n",
        "#define subplot to see graphs side by side\n",
        "fig, ax = plt.subplots(figsize = (10, 5))\n",
        "\n",
        "#create graphs\n",
        "sns.kdeplot(train_data['word_count'][train_data['sentiment'] == 0], shade = True, label = 'Neutral')\n",
        "sns.kdeplot(train_data['word_count'][train_data['sentiment'] == 1], shade = True, label = 'Pro')\n",
        "sns.kdeplot(train_data['word_count'][train_data['sentiment'] == 2], shade = True, label = 'News')\n",
        "sns.kdeplot(train_data['word_count'][train_data['sentiment'] == -1], shade = True, label = 'Anti')\n",
        "\n",
        "#set title and plot\n",
        "plt.title('Distribution of Tweet Word Count')\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Sentiments Proportions')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DW5IS1ahlPdd",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "#create column for the number of characters in a tweet\n",
        "train_data['character_count'] = train_data['message'].apply(lambda x: len(x))\n",
        "\n",
        "#split so we can use updated train set with new feature\n",
        "train_data = train_data[:len(train_data)]\n",
        "\n",
        "#define subplot to see graphs side by side\n",
        "fig, ax = plt.subplots(figsize = (10, 5))\n",
        "\n",
        "#create graphs\n",
        "sns.kdeplot(train_data['character_count'][train_data['sentiment'] == 0], shade = True, label = 'Neutral')\n",
        "sns.kdeplot(train_data['character_count'][train_data['sentiment'] == 1], shade = True, label = 'Pro')\n",
        "sns.kdeplot(train_data['character_count'][train_data['sentiment'] == 2], shade = True, label = 'News')\n",
        "sns.kdeplot(train_data['character_count'][train_data['sentiment'] == -1], shade = True, label = 'Anti')\n",
        "\n",
        "\n",
        "#set title and plot\n",
        "plt.title('Distribution of Tweet Character Count')\n",
        "plt.xlabel('Character Count')\n",
        "plt.ylabel('Sentiment Probability')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AV8g9fpKlPdg",
        "colab": {}
      },
      "source": [
        "#define function to find average word length \n",
        "def average_word_length(x):\n",
        "    x = x.split()\n",
        "    return np.mean([len(i) for i in x])\n",
        "\n",
        "#broadcast to text column\n",
        "train_data['average_word_length'] = train_data['message'].apply(average_word_length)\n",
        "\n",
        "#split so we can use updated train set with new feature\n",
        "train_data = train_data[:len(train_data)]\n",
        "\n",
        "#define subplot to see graphs side by side\n",
        "fig, ax = plt.subplots(figsize = (10, 5))\n",
        "\n",
        "#create graphs\n",
        "sns.kdeplot(train_data['average_word_length'][train_data['sentiment'] == 0], shade = True, label = 'Neutral')\n",
        "sns.kdeplot(train_data['average_word_length'][train_data['sentiment'] == 1], shade = True, label = 'Pro')\n",
        "sns.kdeplot(train_data['average_word_length'][train_data['sentiment'] == 2], shade = True, label = 'News')\n",
        "sns.kdeplot(train_data['average_word_length'][train_data['sentiment'] == -1], shade = True, label = 'Anti')\n",
        "\n",
        "#set title\n",
        "plt.title('Distribution of Tweet Average Word Length')\n",
        "plt.xlabel('Average Word Length')\n",
        "plt.ylabel('Sentiment Probability')\n",
        "\n",
        "#splot graphs\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YPPlB_8plPdl"
      },
      "source": [
        "xxxxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "06jnwSNplPdm"
      },
      "source": [
        "## 4.1 Insights    ##NDUDUZO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j65NzMnaGM2p"
      },
      "source": [
        "##### Analysing the  `label` column in relation to text data\n",
        "xxxxxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "heading_collapsed": true,
        "id": "7cIJ8jmylPdo"
      },
      "source": [
        "##### Conclude\n",
        "By now, we should be so much more familiar with the data. GOOD! Let's go a little further with Exploratory Data Analysis (EDA) withing our initial Feature Engineering/Extraction steps that are to follow. Now that we have an idea of what new features to construct and how they might be useful, let's add the rest of them and visualize them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bXWGq-NGlPdp"
      },
      "source": [
        "# 5. Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oeMHCJ7sTlMu",
        "colab": {}
      },
      "source": [
        "# load train data\n",
        "train_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aw7WHPwcTlM2",
        "colab": {}
      },
      "source": [
        "# load test data\n",
        "test_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/test.csv')\n",
        "print('Test data rows and columns:')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9MG_aBpdTlM7",
        "colab": {}
      },
      "source": [
        "# load sample data\n",
        "sample_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/sample_submission.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-U-2Wg2MU6tl",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "#this is just cool\n",
        "from tqdm import tqdm\n",
        "\n",
        "#visualization\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')   #for optimum aesthetics \n",
        "import seaborn as sns\n",
        "\n",
        "#natural language processing\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gensim\n",
        "import spacy\n",
        "\n",
        "#stemming/lemmatizing/vectorizing\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#processing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#ignore warnings because they are annoying\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from nltk.util import ngrams\n",
        "from collections import defaultdict\n",
        "from collections import  Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d25J72dKlPdq"
      },
      "source": [
        "###### **Task: Generate new features and visualise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tNGBHLPAlPds",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "#add unique word count\n",
        "train_data['unique_word_count'] = train_data['message'].apply(lambda x: len(set(x.split())))\n",
        "\n",
        "#add stopword count\n",
        "stopwords = stopwords.words('english')\n",
        "train_data['stopword_count'] = train_data['message'].apply(lambda x: len([i for i in x.lower().split() if i in stopwords]))\n",
        "\n",
        "#add url count\n",
        "train_data['url_count'] = train_data['message'].apply(lambda x: len([i for i in x.lower().split() if 'http' in i or 'https' in i]))\n",
        "\n",
        "# add hashtag_count\n",
        "train_data['hashtag_count'] = train_data['message'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
        "\n",
        "#add mention count\n",
        "train_data['mention_count'] = train_data['message'].apply(lambda x: len([i for i in str(x) if i == '@']))\n",
        "\n",
        "#add punctuation count\n",
        "train_data['punctuation_count'] = train_data['message'].apply(lambda x: len([i for i in str(x) if i in string.punctuation]))\n",
        "\n",
        "#split so we can use updated train set\n",
        "train_data = train_data[:len(train_data)]\n",
        "\n",
        "disaster = train_data['sentiment'].astype(int) == 1\n",
        "\n",
        "#produce graphs to visualize newly added features\n",
        "fig, axes = plt.subplots(6, figsize=(20, 30))\n",
        "\n",
        "graph1 = sns.kdeplot(train_data.loc[~disaster]['unique_word_count'], shade = True, label = 'Neutral', ax=axes[0])\n",
        "graph1 = sns.kdeplot(train_data.loc[disaster]['unique_word_count'], shade = True, label = 'Pro', ax=axes[0])\n",
        "graph1 = sns.kdeplot(train_data.loc[disaster]['unique_word_count'], shade = True, label = 'News', ax=axes[0])\n",
        "graph1 = sns.kdeplot(train_data.loc[disaster]['unique_word_count'], shade = True, label = 'Anti', ax=axes[0])\n",
        "graph1.set_title('Distribution of Unique Word Count')\n",
        "# plt.xlabel('unique_word_count')\n",
        "\n",
        "graph2 = sns.kdeplot(train_data.loc[~disaster]['stopword_count'], shade = True, label = 'Neutral', ax=axes[1])\n",
        "graph2 = sns.kdeplot(train_data.loc[disaster]['stopword_count'], shade = True, label = 'Pro', ax=axes[1])\n",
        "graph2 = sns.kdeplot(train_data.loc[disaster]['stopword_count'], shade = True, label = 'News', ax=axes[1])\n",
        "graph2 = sns.kdeplot(train_data.loc[disaster]['stopword_count'], shade = True, label = 'Anti', ax=axes[1])\n",
        "graph2.set_title('Distribution of Stopword Word Count')\n",
        "\n",
        "graph3 = sns.kdeplot(train_data.loc[~disaster]['url_count'], shade = True, label = 'Neutral', ax=axes[2])\n",
        "graph3 = sns.kdeplot(train_data.loc[disaster]['url_count'], shade = True, label = 'Pro', ax=axes[2])\n",
        "graph3 = sns.kdeplot(train_data.loc[disaster]['url_count'], shade = True, label = 'News', ax=axes[2])\n",
        "graph3 = sns.kdeplot(train_data.loc[disaster]['url_count'], shade = True, label = 'Anti', ax=axes[2])\n",
        "graph3.set_title('Distribution of URL Count')\n",
        "\n",
        "graph4 = sns.kdeplot(train_data.loc[~disaster]['hashtag_count'], shade = True,  label = 'Neutral', ax=axes[3], bw = 1)\n",
        "graph4 = sns.kdeplot(train_data.loc[disaster]['hashtag_count'], shade = True, label = 'Pro', ax=axes[3], bw =1)\n",
        "graph4 = sns.kdeplot(train_data.loc[disaster]['hashtag_count'], shade = True, label = 'News', ax=axes[3], bw =1)\n",
        "graph4 = sns.kdeplot(train_data.loc[disaster]['hashtag_count'], shade = True, label = 'Anti', ax=axes[3], bw =1)\n",
        "graph4.set_title('Distribution of Hashtag Count')\n",
        "\n",
        "graph4 = sns.kdeplot(train_data.loc[~disaster]['mention_count'], shade = True,  label = 'Neutral', ax=axes[4], bw = 1)\n",
        "graph4 = sns.kdeplot(train_data.loc[disaster]['mention_count'], shade = True, label = 'Pro', ax=axes[4], bw =1)\n",
        "graph4 = sns.kdeplot(train_data.loc[disaster]['mention_count'], shade = True, label = 'News', ax=axes[4], bw =1)\n",
        "graph4 = sns.kdeplot(train_data.loc[disaster]['mention_count'], shade = True, label = 'Anti', ax=axes[4], bw =1)\n",
        "graph4.set_title('Distribution of Mention Count')\n",
        "\n",
        "graph5 = sns.kdeplot(train_data.loc[~disaster]['punctuation_count'], shade = True, label = 'Neutral', ax=axes[5], bw = 1)\n",
        "graph5 = sns.kdeplot(train_data.loc[disaster]['punctuation_count'], shade = True, label = 'Pro', ax=axes[5], bw = 1)\n",
        "graph5 = sns.kdeplot(train_data.loc[disaster]['punctuation_count'], shade = True, label = 'News', ax=axes[5], bw = 1)\n",
        "graph5 = sns.kdeplot(train_data.loc[disaster]['punctuation_count'], shade = True, label = 'Anti', ax=axes[5], bw = 1)\n",
        "graph5.set_title('Distribution of Punctuation Count')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QVMn_YnBlPdv",
        "colab": {}
      },
      "source": [
        "train_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "03JNcBRWlPd1"
      },
      "source": [
        "### 5.1 Further Pre-Processing\n",
        "* Data Cleaning\n",
        "* Using NLP\n",
        "\n",
        "Now that we have explored our data, we need to prepare it for machine learning. In general, to process text we need to apply the following procedure:\n",
        "\n",
        "raw text corpus -> processing text -> tokenized text -> corpus vocabulary -> text representation\n",
        "\n",
        "We can do most of the hard work with Keras's Tokenize object, which automatically converts all words to lowercase and filters out punctuation\n",
        "\n",
        "This tokenizer has many arguements that allow you to do most of the cleaning with one line of code, so we do not need to much processing ourselves. I have included some examples of how one would manually clean text for reference:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ybl0JhLMlPd2"
      },
      "source": [
        "### Create preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oNCsxIaolPd2",
        "colab": {}
      },
      "source": [
        "#remove punctuation\n",
        "def remove_punctuation(message):\n",
        "    return message.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "#remove stopwords\n",
        "StopWords = stopwords\n",
        "def remove_stopwords(message):\n",
        "    return ' '.join([i for i in message.split() if i not in StopWords])\n",
        "\n",
        "#remove words less than 4 \n",
        "def remove_less_than(message):\n",
        "    return ' '.join([i for i in message.split() if len(i) > 3])\n",
        "\n",
        "#remove words with non-alphabet characters\n",
        "def remove_non_alphabet(message):\n",
        "    return ' '.join([i for i in message.split() if i.isalpha() == True])\n",
        "\n",
        "#stem words\n",
        "stemmer = SnowballStemmer('english')\n",
        "def stem_words(message):\n",
        "    return stemmer.stem(message)\n",
        "\n",
        "#lemmatize words for verb\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words_verb(message):\n",
        "    return lemmatizer.lemmatize(message, 'v')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RSebf1gClPd5",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer #in contrast to RegExTokenizer from nltk libarary\n",
        "\n",
        "contractions = [\"can't stop won't stop\"]\n",
        "\n",
        "tokenizer = Tokenizer(filters= \"'!#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\")\n",
        "tokenizer.fit_on_texts(contractions)\n",
        "tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o2K79NW5lPd9",
        "colab": {}
      },
      "source": [
        "def correct_contraction(message):\n",
        "    message = str(message).lower()\n",
        "    message = re.sub(r\"he's\", \"he is\", message)\n",
        "    message = re.sub(r\"there's\", \"there is\", message)\n",
        "    message = re.sub(r\"We're\", \"We are\", message)\n",
        "    message = re.sub(r\"That's\", \"That is\", message)\n",
        "    message = re.sub(r\"won't\", \"will not\", message)\n",
        "    message = re.sub(r\"they're\", \"they are\", message)\n",
        "    message = re.sub(r\"Can't\", \"Cannot\", message)\n",
        "    message = re.sub(r\"wasn't\", \"was not\", message)\n",
        "    message = re.sub(r\"aren't\", \"are not\", message)\n",
        "    message = re.sub(r\"isn't\", \"is not\", message)\n",
        "    message = re.sub(r\"What's\", \"What is\", message)\n",
        "    message = re.sub(r\"i'd\", \"I would\", message)\n",
        "    message = re.sub(r\"should've\", \"should have\", message)\n",
        "    message = re.sub(r\"where's\", \"where is\", message)\n",
        "    message = re.sub(r\"we'd\", \"we would\", message)\n",
        "    message = re.sub(r\"i'll\", \"I will\", message)\n",
        "    message = re.sub(r\"weren't\", \"were not\", message)\n",
        "    message = re.sub(r\"They're\", \"They are\", message)\n",
        "    message = re.sub(r\"let's\", \"let us\", message)\n",
        "    message = re.sub(r\"it's\", \"it is\", message)\n",
        "    message = re.sub(r\"can't\", \"cannot\", message)\n",
        "    message = re.sub(r\"don't\", \"do not\", message)\n",
        "    message = re.sub(r\"you're\", \"you are\", message)\n",
        "    message = re.sub(r\"i've\", \"I have\", message)\n",
        "    message = re.sub(r\"that's\", \"that is\", message)\n",
        "    message = re.sub(r\"i'll\", \"I will\", message)\n",
        "    message = re.sub(r\"doesn't\", \"does not\", message)\n",
        "    message = re.sub(r\"i'd\", \"I would\", message)\n",
        "    message = re.sub(r\"didn't\", \"did not\", message)\n",
        "    message = re.sub(r\"ain't\", \"am not\", message)\n",
        "    message = re.sub(r\"you'll\", \"you will\", message)\n",
        "    message = re.sub(r\"I've\", \"I have\", message)\n",
        "    message = re.sub(r\"Don't\", \"do not\", message)\n",
        "    message = re.sub(r\"I'll\", \"I will\", message)\n",
        "    message = re.sub(r\"I'd\", \"I would\", message)\n",
        "    message = re.sub(r\"Let's\", \"Let us\", message)\n",
        "    message = re.sub(r\"you'd\", \"You would\", message)\n",
        "    message = re.sub(r\"It's\", \"It is\", message)\n",
        "    message = re.sub(r\"Ain't\", \"am not\", message)\n",
        "    message = re.sub(r\"Haven't\", \"Have not\", message)\n",
        "    message = re.sub(r\"Could've\", \"Could have\", message)\n",
        "    message = re.sub(r\"youve\", \"you have\", message)\n",
        "    message = re.sub(r\"haven't\", \"have not\", message)\n",
        "    message = re.sub(r\"hasn't\", \"has not\", message)\n",
        "    message = re.sub(r\"There's\", \"There is\", message)\n",
        "    message = re.sub(r\"He's\", \"He is\", message)\n",
        "    message = re.sub(r\"It's\", \"It is\", message)\n",
        "    message = re.sub(r\"You're\", \"You are\", message)\n",
        "    message = re.sub(r\"I'M\", \"I am\", message)\n",
        "    message = re.sub(r\"shouldn't\", \"should not\", message)\n",
        "    message = re.sub(r\"wouldn't\", \"would not\", message)\n",
        "    message = re.sub(r\"i'm\", \"I am\", message)\n",
        "    message = re.sub(r\"I'm\", \"I am\", message)\n",
        "    message = re.sub(r\"Isn't\", \"is not\", message)\n",
        "    message = re.sub(r\"Here's\", \"Here is\", message)\n",
        "    message = re.sub(r\"you've\", \"you have\", message)\n",
        "    message = re.sub(r\"we're\", \"we are\", message)\n",
        "    message = re.sub(r\"what's\", \"what is\", message)\n",
        "    message = re.sub(r\"couldn't\", \"could not\", message)\n",
        "    message = re.sub(r\"we've\", \"we have\", message)\n",
        "    message = re.sub(r\"who's\", \"who is\", message)\n",
        "    message = re.sub(r\"y'all\", \"you all\", message)\n",
        "    message = re.sub(r\"would've\", \"would have\", message)\n",
        "    message = re.sub(r\"it'll\", \"it will\", message)\n",
        "    message = re.sub(r\"we'll\", \"we will\", message)\n",
        "    message = re.sub(r\"We've\", \"We have\", message)\n",
        "    message = re.sub(r\"he'll\", \"he will\", message)\n",
        "    message = re.sub(r\"Y'all\", \"You all\", message)\n",
        "    message = re.sub(r\"Weren't\", \"Were not\", message)\n",
        "    message = re.sub(r\"Didn't\", \"Did not\", message)\n",
        "    message = re.sub(r\"they'll\", \"they will\", message)\n",
        "    message = re.sub(r\"they'd\", \"they would\", message)\n",
        "    message = re.sub(r\"DON'T\", \"DO NOT\", message)\n",
        "    message = re.sub(r\"they've\", \"they have\", message)\n",
        "    \n",
        "    #correct some acronyms while we are at it\n",
        "    message = re.sub(r\"nba\", \"National Basketball Association\", message)\n",
        "    message = re.sub(r\"azwx\", \"Arizona Weather\", message)  \n",
        "    message = re.sub(r\"alwx\", \"Alabama Weather\", message)\n",
        "    message = re.sub(r\"wordpressdotcom\", \"wordpress\", message)      \n",
        "    message = re.sub(r\"gawx\", \"Georgia Weather\", message)  \n",
        "    message = re.sub(r\"scwx\", \"South Carolina Weather\", message)  \n",
        "    message = re.sub(r\"cawx\", \"California Weather\", message)\n",
        "    message = re.sub(r\"usNWSgov\", \"United States National Weather Service\", message) \n",
        "    message = re.sub(r\"epa\", \"Environmental Protection Agency\", message)\n",
        "    message = re.sub(r\"okwx\", \"Oklahoma City Weather\", message)\n",
        "    message = re.sub(r\"rt\", \"retweet\", message)\n",
        "    message = re.sub(r\"ny\", \"New York\", message)\n",
        "    message = re.sub(r\"arwx\", \"Arkansas Weather\", message)  \n",
        "  \n",
        "    \n",
        "    return message\n",
        "\n",
        "train_data['message'] = train_data['message'].apply(correct_contraction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mjcvRxW6lPeB",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "train_data['message']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qNfV7E1olPeD"
      },
      "source": [
        "### Remove URL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y3v7YasAlPeD",
        "colab": {}
      },
      "source": [
        "example = \"My Profile: https://auth.geeksforgeeks.org\\\n",
        "/ user / Chinmoy % 20Lenka / articles in\\\n",
        "the portal of http://www.geeksforgeeks.org/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6B5hcjL7lPeF",
        "colab": {}
      },
      "source": [
        "def remove_URL(message):\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'',message)\n",
        "\n",
        "remove_URL(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BC_aoIrslPeH",
        "colab": {}
      },
      "source": [
        "train_data['message']=train_data['message'].apply(lambda x : remove_URL(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VoDXpVtrlPeJ"
      },
      "source": [
        "### Remove line breaks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CzhIFmJ8lPeK",
        "colab": {}
      },
      "source": [
        "example = \"Hello \\n World\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C-5aJUkKlPeM",
        "colab": {}
      },
      "source": [
        "print (example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_TkPyQQlPeO",
        "colab": {}
      },
      "source": [
        "def line_break(message):\n",
        "    break_line = message.replace('\\n', ' ')\n",
        "    return break_line\n",
        "\n",
        "line_break(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vqFeADpXlPeQ",
        "colab": {}
      },
      "source": [
        "train_data['message']=train_data['message'].apply(lambda x : remove_URL(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YzQNLhGElPeS"
      },
      "source": [
        "### Remove HTML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L_3jIoPClPeT",
        "colab": {}
      },
      "source": [
        "example = \"\"\"<div>\n",
        "<h1>Reel or Real</h1>\n",
        "<p>Hindustan </p>\n",
        "<a href=\"https://www.hindustan.com/c/nlp-open-the source\">get the source</a>\n",
        "</div>\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h4g1eZAYlPeV",
        "colab": {}
      },
      "source": [
        "def remove_html(message):\n",
        "    html=re.compile(r'<.*?>')\n",
        "    return html.sub(r'',message)\n",
        "print(remove_html(example))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XCP4ORavlPea",
        "colab": {}
      },
      "source": [
        "train_data['message']=train_data['message'].apply(lambda x : remove_html(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K-qw4_uOlPec"
      },
      "source": [
        "### Remove emojis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pQrJ-xP2lPec",
        "colab": {}
      },
      "source": [
        "def remove_emoji(message):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', message)\n",
        "\n",
        "remove_emoji(\"RT @GlblCtzn: 'I don't wanna live forever – and nothing will because climate change' ����️�� @taylor...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y8hHtRQ5lPeh",
        "colab": {}
      },
      "source": [
        "train_data['message']=train_data['message'].apply(lambda x: remove_emoji(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X_s2WMsYlPel"
      },
      "source": [
        "### Remove Puncuations and Capital Letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mx_9piBGlPel",
        "colab": {}
      },
      "source": [
        "def remove_punct(text):\n",
        "    table=str.maketrans('','',string.punctuation)\n",
        "    return text.translate(table)\n",
        "\n",
        "example=\"I am a #GREAT Man.,\"\n",
        "print(remove_punct(example))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-QXJ4Q6glPeo",
        "colab": {}
      },
      "source": [
        "train_data['message']=train_data['message'].apply(lambda x: remove_punct(x).lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hiz2av3clPer"
      },
      "source": [
        "### Remove numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wDihzNF5lPer",
        "colab": {}
      },
      "source": [
        "# remove numbers from text\n",
        "train_data['message']=train_data['message'].apply(lambda x: re.sub('\\w*\\d\\w*', ' ', x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SyH_gBU0lPet"
      },
      "source": [
        "### Remove URL extra spaces as a result of cleaning text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s773hHM8lPet",
        "colab": {}
      },
      "source": [
        "# remove extra spaces\n",
        "train_data['message']=train_data['message'].apply(lambda x: re.sub(' +',' ',x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dJ5ZfRyQxs8Y",
        "colab": {}
      },
      "source": [
        "#remove undetected punctuations and unusual alphabets\n",
        "train_data['message']=train_data['message'].apply(lambda x: re.sub('[“”‘’—…ã¢â‚„¬â¦€]',' ',x))   #Vicky added this, removed all undetected punctuations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Df9s7v93lPev"
      },
      "source": [
        "### Check the spelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C0pN66mblPex",
        "colab": {}
      },
      "source": [
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "spell = SpellChecker()\n",
        "def correct_spellings(text):\n",
        "    corrected_text = []\n",
        "    misspelled_words = spell.unknown(text.split())\n",
        "    for word in text.split():\n",
        "        if word in misspelled_words:\n",
        "            corrected_text.append(spell.correction(word))\n",
        "        else:\n",
        "            corrected_text.append(word)\n",
        "    return \" \".join(corrected_text)\n",
        "        \n",
        "text = \"corect me plese\"\n",
        "correct_spellings(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dV02kjRxlPe2",
        "colab": {}
      },
      "source": [
        "train_data.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WAJNeZdsoWN-"
      },
      "source": [
        "### 5.2 Exploratory Data Analysis on Clean Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EtFOCFjNnsHC"
      },
      "source": [
        "### Clustering of Messages into Pro, News, Neutral and Anti Tweets\n",
        "\n",
        "The different Sentiment tweets are clustered based on the sentiments description to see what is common about the tweets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PKFPsjA1mua8",
        "colab": {}
      },
      "source": [
        "#Create function to compute the news, pro, neutral,anti analysis    #Vicky added this for clustering \n",
        "\n",
        "def getAnalysis(sentiment):\n",
        "    if sentiment == -1:\n",
        "        return 'Anti'\n",
        "    elif sentiment ==0:\n",
        "        return 'Neutral'\n",
        "    elif sentiment == 1:\n",
        "        return 'Pro'\n",
        "    else:\n",
        "        return 'News'\n",
        "\n",
        "train_data['Analysis']= train_data['sentiment'].apply(getAnalysis)\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HvQMTPY8nbG9",
        "colab": {}
      },
      "source": [
        "#All of the News Analysis Tweets\n",
        "news=1\n",
        "sorted_news= train_data.sort_values(by=['sentiment']) \n",
        "for i in range(0, sorted_new.shape[0]): \n",
        "    if sorted_news['Analysis'][i] == 'News':\n",
        "       print(str(news) + '.' + sorted_news['message'][i])\n",
        "       print()\n",
        "       news=news +1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z09etjIQzOiS",
        "colab": {}
      },
      "source": [
        "#All of the Pro Analysis Tweets\n",
        "pro=1\n",
        "sorted_pro= train_data.sort_values(by=['sentiment']) \n",
        "for i in range(0, sorted_pro.shape[0]): \n",
        "    if sorted_pro['Analysis'][i] == 'Pro':\n",
        "       print(str(pro) + '.' + sorted_pro['message'][i])\n",
        "       print()\n",
        "       pro=pro +1\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i0cwBQNB7o_U",
        "colab": {}
      },
      "source": [
        "#\"\"\"df_pro_tweets= pd.DataFrame([message for message in pro_tweets], columns= ['Pro Tweets'])   #tried to create a dataframe but it doesnt want to work\n",
        "#df_pro_tweets\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "smnEhCbH1PX2",
        "colab": {}
      },
      "source": [
        "#All of the Anti Analysis Tweets\n",
        "anti=1\n",
        "sorted_anti= train_data.sort_values(by=['sentiment']) \n",
        "for i in range(0, sorted_anti.shape[0]): \n",
        "    if sorted_anti['Analysis'][i] == 'Anti':\n",
        "       print(str(anti) + '.' + sorted_anti['message'][i])\n",
        "       print()\n",
        "       anti=anti +1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eyGW_91L1p-d",
        "colab": {}
      },
      "source": [
        "#All of the Neutral Analysis Tweets\n",
        "neu=1\n",
        "sorted_neu= train_data.sort_values(by=['sentiment']) \n",
        "for i in range(0, sorted_neu.shape[0]): \n",
        "    if sorted_neu['Analysis'][i] == 'Neutral':\n",
        "       print(str(neu) + '.' + sorted_neu['message'][i])\n",
        "       print()\n",
        "       neu=neu +1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L56Tb0IeAA2V"
      },
      "source": [
        "### Polarity\n",
        "\n",
        "Tells us how positive or negative a text is.\n",
        "\n",
        "It is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement and 0 is neutral.\n",
        "\n",
        "It simply means emotions expressed in a sentence.\n",
        "\n",
        "Emotions are closely related to sentiments. The strength of a sentiment or opinion is typically linked to the intensity of certain emotions, e.g., joy and anger."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wsOhP9SuEViB",
        "colab": {}
      },
      "source": [
        "!pip install textblob\n",
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kmpaxiEc_-6v",
        "colab": {}
      },
      "source": [
        "#Add Polarity on the train_data\n",
        "def GetPolarity(text):\n",
        "  return TextBlob(text).sentiment.polarity \n",
        "\n",
        "train_data['Polarity']= train_data['message'].apply(GetPolarity)\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iRJ6SDtvHR4a"
      },
      "source": [
        "###Subjectivity\n",
        "\n",
        "Tells us how subjective or opinionated a text is.\n",
        "\n",
        "Sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qZP-tY_aH0ja",
        "colab": {}
      },
      "source": [
        "#Add Subjectivity on the train_data\n",
        "def GetSubjectivity(text):\n",
        "  return TextBlob(text).sentiment.subjectivity \n",
        "\n",
        "train_data['Subjectivity']= train_data['message'].apply(GetSubjectivity)\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BqclaJW9RBjm",
        "colab": {}
      },
      "source": [
        "#Plot the Subjectivity vs the Polarity\n",
        "plt.figure(figsize=(8,6))\n",
        "#for i in range(0, train_data.shape[0]):\n",
        "plt.scatter(train_data['Polarity'][:1000], train_data['Subjectivity'][:1000], color = 'purple')\n",
        "\n",
        "plt.title('Sentiment Analysis')\n",
        "plt.xlabel('Polarity')\n",
        "plt.ylabel('Subjectivity')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0DnsUZ863iI-"
      },
      "source": [
        "### WordCloud\n",
        "\n",
        "It is also known as a text cloud and it is a visualization where more specific word appears\n",
        "\n",
        "It is a technique to show which words are the most frequent among the given text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AWnzkrna3ezb",
        "colab": {}
      },
      "source": [
        "#plot a word cloud\n",
        "from wordcloud import WordCloud\n",
        "Allwords= ' '.join( [tweets for tweets in train_data['message']] )\n",
        "wordCloud= WordCloud(width= 700, height= 500, random_state= 21, max_font_size= 150).generate(Allwords)\n",
        "plt.imshow(wordCloud, interpolation= 'bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "71C4f1-ehlvx",
        "colab": {}
      },
      "source": [
        "#We will use word_tokenizer to tokenize the clean texts\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "train_data['Tokens']= train_data['message'].apply(lambda word: word_tokenize(word))\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "moBB3edxU9v5"
      },
      "source": [
        "### Bag of words\n",
        "\n",
        "'BOW' is a method to extract features from text documents. These features can be used for training machine learning algorithms. It creates a vocabulary of all the unique words occurring in all the documents in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FuCtbFgXdG7x",
        "colab": {}
      },
      "source": [
        "# bag of words\n",
        "def bag_of_words_count(words, word_dict={}):\n",
        "    \"\"\" this function takes in a list of words and returns a dictionary \n",
        "        with each word as a key, and the value represents the number of \n",
        "        times that word appeared\"\"\"\n",
        "    for word in words:\n",
        "        if word in word_dict.keys():\n",
        "            word_dict[word] += 1\n",
        "        else:\n",
        "            word_dict[word] = 1\n",
        "    return word_dict\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wlUcX1wyepL0",
        "colab": {}
      },
      "source": [
        "bag_words = {}\n",
        "for pp in train_data['sentiment']:\n",
        "    df = train_data.groupby('sentiment')\n",
        "    bag_words[pp] = {}\n",
        "    for row in train_data['Tokens'][:1000]:\n",
        "        bag_words[pp] = bag_of_words_count(row, bag_words[pp]) \n",
        "\n",
        "bag_words     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTFs4RC-ii6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#word count for only Pro\n",
        "import collections\n",
        "def count_words(input):\n",
        "    cnt = collections.Counter()\n",
        "    for row in input:\n",
        "        for word in row:\n",
        "            cnt[word] += 1\n",
        "    return cnt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWALenUHvEs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data[(train_data.Analysis == 'Pro')][['Tokens']].apply(count_words)['Tokens'].most_common(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxxZO7RFyawF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data[(train_data.Analysis == 'Anti')][['Tokens']].apply(count_words)['Tokens'].most_common(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AWC9ndFuaugw"
      },
      "source": [
        "## Summary Feedback\n",
        "\n",
        "On the WordCloud, words like Climate Change, Global Warming, Retweet, Protection Agency are the most frequent in the text. Bag of words shows the most frequent words and it corresponds to the wordcloud\n",
        "\n",
        "We can see that if polarity is for example 0.8, which means that the statement is positive and 0.8 subjectivity refers that mostly it is a public opinion and not a factual information. xxxxcan we analysis the scatter plot "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJAEFCskjThY",
        "colab_type": "text"
      },
      "source": [
        "###Testing Base Model   #done by vicky"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HZL2CAOiLjU_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "78eeb51a-7078-4cf3-8fb6-93abb6cb5054"
      },
      "source": [
        "# import packages   #NDU \n",
        "# from comet_ml import Experiment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "# set plot style\n",
        "sns.set()\n",
        "import re\n",
        "import string\n",
        "import emoji\n",
        "import spacy\n",
        "import sklearn.feature_extraction.text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Importing wordcloud for plotting word clouds and textwrap for wrapping longer text\n",
        "from wordcloud import WordCloud\n",
        "from textwrap import wrap\n",
        "#from googletrans import Translator as translator\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn import metrics\n",
        "from nltk.corpus import stopwords\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8x3on-juLjVY",
        "colab": {}
      },
      "source": [
        "# load train data\n",
        "train_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/train.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KlJwaATOLjVh",
        "colab": {}
      },
      "source": [
        "# load test data\n",
        "test_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/test.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2KM5eBJQLjVr",
        "colab": {}
      },
      "source": [
        "# load sample data\n",
        "sample_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/sample_submission.csv')\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MpAXbjKLMJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#combine so we work smarter, not harder\n",
        "total = train_data.append(test_data)\n",
        "\n",
        "#change target to target1 in preparation for the word 'target' to be encoded as its own column\n",
        "total = total.rename(columns = {'sentiment':'sentiment1'})\n",
        "\n",
        "#create column for the number of words in tweet\n",
        "total['word count'] = total['message'].apply(lambda x: len(x.split()))\n",
        "\n",
        "total['character count'] = total['message'].apply(lambda x: len(x))\n",
        "\n",
        "#split so we can use updated train set with new feature\n",
        "train_data = total[:len(train_data)]\n",
        "\n",
        "  \n",
        "#add unique word count\n",
        "total['unique word count'] = total['message'].apply(lambda x: len(set(x.split())))\n",
        "\n",
        "\n",
        "#add url count\n",
        "total['url count'] = total['message'].apply(lambda x: len([i for i in x.lower().split() if 'http' in i or 'https' in i]))\n",
        "\n",
        "#add mention count\n",
        "total['mention count'] = total['message'].apply(lambda x: len([i for i in str(x) if i == '@']))\n",
        "\n",
        "#add hashtag count\n",
        "#total['hashtag count'] = total['text'].apply(lambda x: len([i for i in str(x) if i == '#']))\n",
        "\n",
        "#add punctuation count\n",
        "total['punctuation count'] = total['message'].apply(lambda x: len([i for i in str(x) if i in string.punctuation]))\n",
        "\n",
        "#split so we can use updated train set\n",
        "train_data = total[:len(train_data)]\n",
        "\n",
        "disaster = train_data['sentiment1'] == 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a37eVNbIMNBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove punctuation\n",
        "def remove_punctuation(message):\n",
        "    return message.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "#remove stopwords\n",
        "StopWords = stopwords\n",
        "def remove_stopwords(message):\n",
        "    return ' '.join([i for i in message.split() if i not in StopWords])\n",
        "\n",
        "#remove words less than 4 \n",
        "def remove_less_than(message):\n",
        "    return ' '.join([i for i in message.split() if len(i) > 3])\n",
        "\n",
        "#remove words with non-alphabet characters\n",
        "def remove_non_alphabet(message):\n",
        "    return ' '.join([i for i in message.split() if i.isalpha() == True])\n",
        "\n",
        "#stem words\n",
        "stemmer = SnowballStemmer('english')\n",
        "def stem_words(message):\n",
        "    return stemmer.stem(message)\n",
        "\n",
        "#lemmatize words for verb\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words_verb(message):\n",
        "    return lemmatizer.lemmatize(message, 'v')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgZAzFro9Wbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = train_data['message']  # this time we want to look at the text\n",
        "y = train_data['sentiment1']\n",
        "\n",
        "#X_transform = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz3votVRyah2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
        "\n",
        "\n",
        "count_vectorizer=CountVectorizer(min_df=1, max_df=0.9, stop_words='english', decode_error='ignore')\n",
        "count_vectorized=count_vectorizer.fit_transform(X_train)\n",
        "\n",
        "import scipy.sparse\n",
        "\n",
        "X = scipy.sparse.hstack([X_train_tfidf, count_vectorized])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oscLndTfkg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model classifier\n",
        "from sklearn.svm import LinearSVC\n",
        "clf = LinearSVC()\n",
        "clf.fit(X, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zbMLmY4XHp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.svm import LinearSVC\n",
        "\n",
        "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
        "                     ('clf', LinearSVC()),\n",
        "])\n",
        "\n",
        "# Feed the training data through the pipeline\n",
        "text_clf.fit(X_train, y_train) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSu4vAqI4a-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.confusion_matrix(y_test,predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjJMQRoH4reN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = text_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu0zkWirG18j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(metrics.classification_report(y_test,predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v7BU62EBpTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(metrics.accuracy_score(y_test,predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYVlER3Gfkeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model= LogisticRegression()\n",
        "\n",
        "model.fit(X,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONpNgVANU8kS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.svm import LinearSVC\n",
        "\n",
        "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
        "                     ('model', LogisticRegression()),\n",
        "])\n",
        "\n",
        "# Feed the training data through the pipeline\n",
        "text_clf.fit(X_train, y_train) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh5K00E_fkQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J58taEyjjf6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0scjHT6jf0s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtwKOuW2jfzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JTr2uTbjfk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU8QW369jfiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tnjDkwVQrGn7"
      },
      "source": [
        "# 6. Model Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BUd4WfSelPfF",
        "colab": {}
      },
      "source": [
        "def get_top_tweet_unigrams(corpus, n=None):\n",
        "    vec = CountVectorizer(ngram_range=(6, 6)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TjIskwkjlPfM",
        "colab": {}
      },
      "source": [
        "#from sklearn.model_selection import train_test_split\n",
        "#X_transformed = cv.fit_transform((train_data['lemmatized'])\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.33, random_state=42)\n",
        "#X_raw.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hbbyv-dTlPfO",
        "colab": {}
      },
      "source": [
        "#Option 1: Count vectorization\n",
        "#from sklearn.model_selection import train_test_split\n",
        "#X_train, X_test, y_train, y_test = train_test_split(text_counts, train_data['sentiment'], stratify=train_data['sentiment'], test_size=0.67, random_state=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1wSZ2_E7lPfS",
        "colab": {}
      },
      "source": [
        "#from sklearn.model_selection import train_test_split\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "re5lMqNMlPfV",
        "colab": {}
      },
      "source": [
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#vectorizer = TfidfVectorizer()\n",
        "\n",
        "#X_train_tfidf = vectorizer.fit_transform(X_train) \n",
        "#X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "#X_raw = train_data.drop('sentiment', axis=1)\n",
        "#y_raw = train_data['sentiment']\n",
        "\n",
        "#X_train_tfidf.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ksYhpk03lPfY"
      },
      "source": [
        "###### Task: Split the data into train & test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wRilqqxelPfZ",
        "colab": {}
      },
      "source": [
        "#Option 2: TFIDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf=TfidfVectorizer()\n",
        "text_tf = tf.fit_transform(train_data['message'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fLzOdxSnGM2u",
        "colab": {}
      },
      "source": [
        "#Option 2: TFIDF\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    text_tf, train_data['sentiment1'], test_size=0.67, random_state=42)\n",
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HJwCxCXHlPfd",
        "colab": {}
      },
      "source": [
        "#Option 3: TFIDF\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = train_data['message']  # this time we want to look at the text\n",
        "y = train_data['sentiment1']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ODPcEwVKlPfh",
        "colab": {}
      },
      "source": [
        "#Option 3: TFIDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YC60j1VbGM22"
      },
      "source": [
        "##### Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6EGK-osWGM22"
      },
      "source": [
        "###### **Task: Minimal Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mz_Cl4MUGM3G",
        "colab": {}
      },
      "source": [
        "# code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b5gCDgQ6GM3Q"
      },
      "source": [
        "###### **Task: Train a Logistic Regression Classifier**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZlDKasiTObSl",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_model = LogisticRegression(C=1.0, solver='lbfgs', class_weight=None, multi_class='auto')\n",
        "\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "pred_lr = lr_model.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iiWCv2galPfq"
      },
      "source": [
        "###### **Task: Train a Naive Bayes Regression classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tsvg8PdIlPfq",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "# Model Generation Using Multinomial Naive Bayes\n",
        "clf = MultinomialNB().fit(X_train, y_train)\n",
        "pred_nb = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TMakwg7dlPfs"
      },
      "source": [
        "###### **Task: Train a Support Vector Machine (SVM) Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LQ8kXS9DlPfs",
        "colab": {}
      },
      "source": [
        "# Form a prediction set\n",
        "from sklearn.svm import LinearSVC\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_train, y_train)\n",
        "pred_lsvc = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "69Cwmx-glPfw",
        "colab": {}
      },
      "source": [
        "# Form a prediction set\n",
        "#from sklearn.svm import LinearSVC\n",
        "#clf = LinearSVC()\n",
        "#clf.fit(X_train_tfidf, y_train)\n",
        "#pred_lsvc = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YeyZwFptlPfy",
        "colab": {}
      },
      "source": [
        "#from sklearn.svm import LinearSVC\n",
        "\n",
        "#lsvc_model = LinearSVC()\n",
        "\n",
        "#lsvc_model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n8Tss1onlPf0"
      },
      "source": [
        "### 6.1 Initial model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ExK08O3TlPf0"
      },
      "source": [
        "###### **task: test the Accuracy of the model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jFn1pR3BlPf0",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WkznqVNvlPf2",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o5BzTjNxlPf4",
        "colab": {}
      },
      "source": [
        "print(\"SVC Accuracy:\",metrics.accuracy_score(y_test, pred_lsvc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7x_rea8YlPf7",
        "colab": {}
      },
      "source": [
        "print(metrics.confusion_matrix(y_test,pred_lsvc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v9ddn5EvlPf9",
        "colab": {}
      },
      "source": [
        "print(metrics.classification_report(y_test,pred_lsvc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "73yS1r8cGM3X",
        "colab": {}
      },
      "source": [
        "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, pred_nb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ziEzuH9UlPgE",
        "colab": {}
      },
      "source": [
        "print(metrics.confusion_matrix(y_test,pred_nb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4TUQKFVDlPgG",
        "colab": {}
      },
      "source": [
        "print(metrics.classification_report(y_test,pred_nb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DpUJd685lPgI",
        "colab": {}
      },
      "source": [
        "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, pred_lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jkBGO8ralPgK",
        "colab": {}
      },
      "source": [
        "print(metrics.confusion_matrix(y_test,pred_lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JeUC7M8HlPgN",
        "colab": {}
      },
      "source": [
        "labels = ['0: Neutral', '1: Pro', '2:News', '-1:Anti']\n",
        "\n",
        "pd.DataFrame(data=confusion_matrix(y_test, pred_lr), index=labels, columns=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8dniF8MklPgP",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Saving each metric to add to a dictionary for logging to comet\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "precision = precision_score(y_test, pred_lr, labels=None, average='macro')\n",
        "recall = recall_score(y_test, pred_lr, labels=None, average='macro')\n",
        "f1 = f1_score(y_test, pred_lr, labels=None, average='macro')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bzy_34DVlPgQ",
        "colab": {}
      },
      "source": [
        "# Create dictionaries for the comet data we want to log\n",
        "\n",
        "#params = {\"random_state\": 7,\n",
        "          #\"model_type\": \"logreg\",\n",
        "          #\"scaler\": \"standard scaler\",\n",
        "          #\"param_grid\": str(param_grid),\n",
        "          #\"stratify\": True\n",
        "          #}\n",
        "\n",
        "metrics = {\"f1\": f1,\n",
        "           \"recall\": recall,\n",
        "           \"precision\": precision\n",
        "           }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KRVuytoGlPgS",
        "colab": {}
      },
      "source": [
        "# Log our comet parameters and results\n",
        "#experiment.log_parameters(params)\n",
        "#experiment.log_metric(\"accuracy\", f1)\n",
        "print(\"Logistic regression f1 macro score metrics: \", metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A5MywCwklPgT",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "print(metrics.classification_report(y_test,pred_lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jIlAselulPgV",
        "colab": {}
      },
      "source": [
        "experiment.end()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vD-jIGvElPgW"
      },
      "source": [
        "###### **task: display experiment on comet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vw8Yxc8ilPgX",
        "colab": {}
      },
      "source": [
        "experiment.display()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2W_ZOB_JlPgY"
      },
      "source": [
        "###### **task: save output of highest performing model results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MFAOEYtzlPgY",
        "colab": {}
      },
      "source": [
        "sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "znPSm6qLlPga",
        "colab": {}
      },
      "source": [
        "clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S8SZoIgUlPgc",
        "colab": {}
      },
      "source": [
        "lr_model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T-QZhwYblPge",
        "colab": {}
      },
      "source": [
        "y_pred= pd.DataFrame(pred_lsvc).astype(int)\n",
        "base_df = pd.DataFrame()\n",
        "base_df['tweetid'] = test_data['tweetid']\n",
        "base_df['sentiment'] = y_pred\n",
        "sample_data['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q73BQj-elPgf",
        "colab": {}
      },
      "source": [
        "#base_df.to_csv('/kaggle/working/submissionv4.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iOGU3Q2ilPgh",
        "colab": {}
      },
      "source": [
        "base_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SIR87MU-lPgu"
      },
      "source": [
        "###Summarise findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_sT1UmDGHQxq"
      },
      "source": [
        "# 7. Predictive Modelling\n",
        "xxxxxx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_rkIlq-VFn1g"
      },
      "source": [
        "### 7.1 An Overview of learners\n",
        "**List of models we will train are as follows:**\n",
        "\n",
        "\n",
        "### Logistic Regression\n",
        "\n",
        "Logistic regression is used to obtain odds ratio in the presence of more than one explanatory variable (it explains the relationship between one dependent binary variable and one or more independent variables). Futher more it is used to describe data. The procedure is quite similar to multiple linear regression, with the exception that the response variable is binomial(has a dependent variable with two possible values labeled 0 and 1). The result is the impact of each variable on the odds ratio of the observed event of interest. The main advantage is to avoid confounding effects by analyzing the association of all variables together. A disadvantage of this model may be overfitting whereby too many variables are added, which reduces the generalizability of the model beyond the data on which the model is fit. In this article, we explain the logistic regression procedure using examples to make it as simple as possible. (https://www.researchgate.net/publication/260810482_Understanding_logistic_regression_analysis)\n",
        "\n",
        "### K-Nearest Neighbours\n",
        "\n",
        "K-nearest neighbors (KNN) is a powerful, yet easy to understand machine learning algorithm. It relies on labeled input data to learn a function that produces an appropriate output when given new unlabeled data. In principle, this algorithm works by assigning the majority class of the N closest neighbors to the currect data point(it captures the idea of similarity). As such, absolutely no training is required for the algorithm! All we do is choose K (i.e. the number of neighbors to consider), choose a distance function to calculate proximity and we're good to go. As we decrease the value of K to 1, our predictions become less stable. Inversely, as we increase the value of K, our predictions become more stable making it more likely to make more accurate predictions to a certain point. where we will, eventually begin to witness an increasing number of errors. It is at this point we know we have pushed the value of K too far. The algorithm gets significantly slower as the number of examples and/or predictors/independent variables increase.\n",
        "(https://athena.explore-datascience.net/student/content/train-view/38/100/1783)\n",
        "\n",
        "### Support Vector Machines \n",
        "\n",
        "Support Vector Machines (SVMs) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. It separates data points using a hyperplane with the largest amount of margin. That's why an SVM classifier is also known as a discriminative classifier. The Support Vector Machine (SVM) algorithm is a popular machine learning tool that offers solutions for both classification and regression problems. Like logistic regression, they fit a linear decision boundary. However, unlike logistic regression, SVMs do this in a non-proabilistic way and are able to fit to non-linear data using an algorithm known as the kernel trick. In sklearn, these are called SVC (Support Vector Classifier) and SVR (Support Vector Regression) respectively. Classification of images can also be performed using SVM. (https://athena.explore-datascience.net/student/content/train-view/38/100/1783)\n",
        "\n",
        "### Naïve Bayes\n",
        "\n",
        "Naive Bayes is a classification algorithm that uses the principle of Bayes theorem to make classifications. The benefits of Naive Bayes are that the model is simple to build and is useful on large data sets. Further, it only requires a small number of training data to estimate the parameters necessary for classification. However the model makes an explicit assumption that the features are independent given the class label making it almost impossible to get a set of predictors which are completely independent. .(https://athena.explore-datascience.net/student/content/train-view/38/100/1783)\n",
        "\n",
        "### Decision Tree\n",
        "\n",
        "The Decision tree model uses a non parameterised approach to making predictions. Its basically a series of questions that the algorithm asks about a particular sample. For example, if the value is this then the target is that. The algorithm starts off by selecting a variable that produces the best split of the data. Every predictor is assigned an impurity scores based on how accurate its predictions are (measured by rmse). The feature with the lowest impurity score is used for the initial split at the top of the tree, know as the root node. The other variables are used for splits down the tree all the way to the final predictions at the leaf nodes. Trees can easily overfit the training data because of their slightly more complex nature compared with linear models. Therefore it is common to build a tree to maximum depth and then prune it by getting rid of branches that do not necessarily result in significantly lower rmse in the next node. Finally, tree can easily decipher non linear patterns in dataset and thusare often useful for non linearly distributed datasets. \n",
        "\n",
        "### Random Forest\n",
        "\n",
        "Random Forest is an extension of decision trees. Single decision tree often have high variance. Their predictions depend a lot on the data they were trained on. A slight change to the training set, their predictions change significantly. Because of this a popular technique is on of building multiple trees and averaging their predictions. This is known as bootstrap aggregation, meaning multiple models are trained on different subsets of the training data. The subsets are obtained through a bootstrap sampling approach where samples can be randomly selected more than once until the length of the bootstrap is equal to the length of the original training set. Note that some of the samples are left out (refered to as out of bag samples). The model is then trained on the bootstrap sample. Random forest takes sampling to yet another level by also using a subset of the predictors to build each tree. This results in not so highly correlated tree in the forest that if averaged can produce stable predictions with little variance. The user has the option to choose the number of tree of treat it as a hyper-paramer to be determined by the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g2i5ODUlPM59"
      },
      "source": [
        "### 7.2 An explanation of Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PcF1pIsHQKIr"
      },
      "source": [
        "A machine learning pipeline is used to help automate machine learning workflows.  They operate by enabling a sequence of data to be transformed and correlated together in a model that can be tested and evaluated to achieve an outcome, whether positive or negative.\n",
        "Machine learning pipelines are cyclical and iterative as every step is repeated to continuously improve the accuracy of the model and achieve a successful algorithm.  To build better machine learning models, and get the most value from them, accessible, scalable and durable storage solutions are imperative, paving the way for on-premises object storage.\n",
        "(https://www.datanami.com/2018/09/05/how-to-build-a-better-machine-learning-pipeline/#:~:text=A%20machine%20learning%20pipeline%20is,outcome%2C%20whether%20positive%20or%20negative.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NQSGWZcAUETR"
      },
      "source": [
        "### 7.3 Build a pipeline to vectorize the data, then train and fit a model\n",
        "xxxx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "igV96lHaF2gh",
        "colab": {}
      },
      "source": [
        "#code here\n",
        "#from sklearn.pipeline import Pipeline\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from sklearn.svm import LinearSVC\n",
        "\n",
        "#text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
        "#                     ('clf', LinearSVC()),\n",
        "#])\n",
        "\n",
        "# Feed the training data through the pipeline\n",
        "#text_clf.fit(X_train, y_train)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q221LNzwYS2v"
      },
      "source": [
        "### 7.4 Run predictions and analyze the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9c8Yma6N0PNI",
        "colab": {}
      },
      "source": [
        "#code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "heading_collapsed": true,
        "id": "0OG-HJfcGM_w"
      },
      "source": [
        "# 8. Feature Selection and Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oFj1d73TdudP"
      },
      "source": [
        "xxxxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uhvcJv0Mi1LT"
      },
      "source": [
        "The code cell below applies the function over multiple Variance Thresholds. We chose value between 0 and 50, increasing exponentially. See the cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UugqmP-4h3sD"
      },
      "source": [
        "###### **Task: Perform Vector Arithmetic between features**\n",
        "xxxxx  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y6ez2svHh2f-",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qj26PJDt-mpV"
      },
      "source": [
        "###Summarise findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kC8CHFk0iviq"
      },
      "source": [
        "##### **Reguarisation - Improving model perfomance**\n",
        "xxxx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "24QgGCFIlPhR",
        "colab": {}
      },
      "source": [
        "`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J5Bkw1RVj0h9"
      },
      "source": [
        "###### **Task: Implement L1 & L2 Regularisation for Logistic Regression**\n",
        "xxxxxx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4vR7XB3tjymJ",
        "colab": {}
      },
      "source": [
        "# code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XoYZOl2HzXLV"
      },
      "source": [
        "###### Task: Select the best variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aA6xggtEUJhd",
        "colab": {}
      },
      "source": [
        "# code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nYuvvuG5lPhf"
      },
      "source": [
        "### 8.1 Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7JvL_s0Gz1nC"
      },
      "source": [
        "###### Task: Adjust Hyperparameter/s of chosen model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ABXGqtLElPhg"
      },
      "source": [
        "xxxxx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E_9CCO6alPhg",
        "colab": {}
      },
      "source": [
        "# code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S0lS1mydlPhi"
      },
      "source": [
        "### Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gtbYpEIjlPhk"
      },
      "source": [
        "###### Task: Select the best model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g6A_MG7qslZP"
      },
      "source": [
        "xxxxxx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JEBgD4Xf2-_S",
        "colab": {}
      },
      "source": [
        "# code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JcsFJf6OGM__"
      },
      "source": [
        "# 9. Summary of Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PJn2DGXyt1C2"
      },
      "source": [
        "\n",
        "\n",
        "## Data Exploration\n",
        "xxxxx\n",
        "## Exploratory data analysis\n",
        "\n",
        "xxxxxx\n",
        "\n",
        "## Predictive modelling\n",
        "\n",
        "xxxxxx\n",
        "\n",
        "\n",
        "## Feature selection\n",
        "\n",
        "xxxxxx\n",
        "\n",
        "\n",
        "\n",
        "## Recommendations\n",
        "\n",
        "xxxxxx\n",
        "\n",
        "\n",
        "## Key takeaways\n",
        "xxxxxx\n"
      ]
    }
  ]
}