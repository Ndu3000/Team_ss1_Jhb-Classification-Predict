{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Updated notebook Team_ss1_JHB.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/blob/master/Updated_notebook_Team_ss1_JHB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nzbv64tkF16w"
      },
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Context\" data-toc-modified-id=\"Context-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Context</a></span></li><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Problem Statement</a></span></li><li><span><a href=\"#Data-sets\" data-toc-modified-id=\"Data-sets-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Data sets</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#Variable-definitions\" data-toc-modified-id=\"Variable-definitions-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Variable definitions</a></span></li><li><span><a href=\"#The-data-input-files-we-have-used-for-our-model-are:\" data-toc-modified-id=\"The-data-input-files-we-have-used-for-our-model-are:-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span>The data input files we have used for our model are:</a></span></li></ul></li></ul></li><li><span><a href=\"#Initial-Data-Exploration\" data-toc-modified--2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Initial Data Exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Import-packages-and-load-data-files\" data-toc-modified-id=\"Import-packages-and-load-data-files-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Import packages and load data files</a></span></li><li><span><a href=\"#Missing-Values\" data-toc-modified-id=\"Missing-Values-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Missing Values</a></span></li></ul></li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Exploratory Data Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Insights\" data-toc-modified-id=\"Insights-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Insights</a></span></li></ul></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Feature Engineering</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initial-model-evaluation\" data-toc-modified-id=\"Initial-model-evaluation-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Initial model evaluation</a></span></li><li><span><a href=\"#Further-Feature-Extraction\" data-toc-modified-id=\"Further-Feature-Extraction-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Further Feature Extraction</a></span></li></ul></li><li><span><a href=\"#Predictive-Modelling\" data-toc-modified-id=\"Predictive-Modelling-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Predictive Modelling</a></span><ul class=\"toc-item\"><li><span><a href=\"#An-Overview-of-learners\" data-toc-modified-id=\"An-Overview-of-learners-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>An Overview of learners</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"#K-Nearest-Neighbours\" data-toc-modified-id=\"K-Nearest-Neighbours-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>K-Nearest Neighbours</a></span></li><li><span><a href=\"#Support-Vector-Machines\" data-toc-modified-id=\"Support-Vector-Machines-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Support Vector Machines</a></span></li><li><span><a href=\"#Naïve-Bayes\" data-toc-modified-id=\"Naïve-Bayes-5.1.4\"><span class=\"toc-item-num\">5.1.4&nbsp;&nbsp;</span>Naïve Bayes</a></span></li><li><span><a href=\"#Decision-Tree\" data-toc-modified-id=\"Decision-Tree-5.1.5\"><span class=\"toc-item-num\">5.1.5&nbsp;&nbsp;</span>Decision Tree</a></span></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-5.1.6\"><span class=\"toc-item-num\">5.1.6&nbsp;&nbsp;</span>Random Forest</a></span></li></ul></li><li><span><a href=\"#An-Overview-of-the-features\" data-toc-modified-id=\"An-Overview-of-the-features-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>An Overview of the features</a></span></li><li><span><a href=\"#An-explanation-of-Pipelines\" data-toc-modified-id=\"An-explanation-of-Pipelines-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>An explanation of Pipelines</a></span></li><li><span><a href=\"#Build-a-pipeline-to-vectorize-the-data,-then-train-and-fit-a-model\" data-toc-modified-id=\"Build-a-pipeline-to-vectorize-the-data,-then-train-and-fit-a-model-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Build a pipeline to vectorize the data, then train and fit a model</a></span></li><li><span><a href=\"#Run-predictions-and-analyze-the-results\" data-toc-modified-id=\"Run-predictions-and-analyze-the-results-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Run predictions and analyze the results</a></span></li></ul></li><li><span><a href=\"#Feature-Selection-and-Model-Selection\" data-toc-modified-id=\"Feature-Selection-and-Model-Selection-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Feature Selection and Model Selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameter-tuning\" data-toc-modified-id=\"Hyperparameter-tuning-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Hyperparameter tuning</a></span></li><li><span><a href=\"#Model-Evaluation\" data-toc-modified-id=\"Model-Evaluation-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Model Evaluation</a></span></li></ul></li><li><span><a href=\"#Summary-of-Conclusions\" data-toc-modified-id=\"Summary-of-Conclusions-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Summary of Conclusions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Exploration\" data-toc-modified-id=\"Data-Exploration-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Data Exploration</a></span></li><li><span><a href=\"#Exploratory-data-analysis\" data-toc-modified-id=\"Exploratory-data-analysis-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Exploratory data analysis</a></span></li><li><span><a href=\"#Predictive-modelling\" data-toc-modified-id=\"Predictive-modelling-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Predictive modelling</a></span></li><li><span><a href=\"#Feature-selection\" data-toc-modified-id=\"Feature-selection-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Feature selection</a></span></li><li><span><a href=\"#Recommendations\" data-toc-modified-id=\"Recommendations-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Recommendations</a></span></li><li><span><a href=\"#Key-takeaways\" data-toc-modified-id=\"Key-takeaways-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>Key takeaways</a></span></li></ul></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i7XMGIMFkyOb"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rJ0GgUJUObLr"
      },
      "source": [
        "# 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "heading_collapsed": true,
        "id": "pP7AYi2lGM09"
      },
      "source": [
        "## 1.1 Context\n",
        "\n",
        "Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cLFB14bwTMrk"
      },
      "source": [
        "## 1.2 Problem Statement\n",
        "\n",
        "With this context, we are being challenged with the task of creating a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n",
        "\n",
        "Providing an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.\n",
        "\n",
        "Our quest is to provide valuable sentiment analysis information with actionable insights that can be used for an entity that  could like its brand to be viewed as \"eco-friendly\" from the perspective of all its stakeeholders e.g. clients, employeers, investors etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5rcVGaPfAA0I"
      },
      "source": [
        "## 1.3 Data Description\n",
        "\n",
        "### 1.3.1 Data\n",
        "\n",
        "The collection of this data which we use as our data source was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo, which was provided by Kaggle. The dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. In total, 43943 tweets were collected. Each tweet is labelled as one of the following classes in the table below:\n",
        "\n",
        "<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F2205222%2F8e4d65f2029797e0462b52022451829c%2Fdata.PNG?generation=1590752860255531&amp;alt=media\" alt=\"\" title=\"\">\n",
        "\n",
        "### 1.3.2 Variable definitions\n",
        "\n",
        "*   sentiment: Sentiment of tweet\n",
        "\n",
        "*   message: Tweet body\n",
        "\n",
        "*   tweetid: Twitter unique id\n",
        "\n",
        "\n",
        "### 1.3.3 The data input files we have used for our model are:\n",
        "\n",
        "*   Train.csv (is the dataset that we will use to train to our model) as denoted by the \"train_data\" dataframe variable in our code.\n",
        "*   Test.csv (is the dataset on which we will apply to our model to) as denoted by the \"test_data\" dataframe variable in our code.\n",
        "*   SampleSubmission.csv (is an example of what our submission file will look like. The order of the rows is not so relevant, but the names of the tweetid's must be correct.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xms0kqxDGM0-"
      },
      "source": [
        "# 2. Download, Import Packages and Load Data files "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gU9fdfaGGM0_"
      },
      "source": [
        "###### Task: Download and install external libraries/packages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vQSytCTMwJnN"
      },
      "source": [
        "### 2.1 Install relevant Libraries\n",
        "\n",
        "We start off by importing relevant packages and loading the data.\n",
        "\n",
        "`Comet:`For viewing different versions of your machinle learning model results - `pip install comet_ml--3.1.11`\n",
        "\n",
        "`Emoji:` For removing emojis - `pip install emoji--0.5.4`\n",
        "\n",
        "`Wordcloud:`For creating a Word Cloud infographic to visualise frequent words used in text - `pip install wordcloud --1.7.0`\n",
        "\n",
        "`Textblob:`For simple, Pythonic text processing. Sentiment analysis, part-of-speech tagging, noun phrase parsing, and more - `pip install textblob--0.15.3` \n",
        "\n",
        "`Natural Language Toolkit (NLTK):`For common natural language processing (NLP) tasks and more - `pip install nltk--3.4.5`\n",
        "\n",
        "`Spacy:`For industrial strength Natural Language Processing (NLP) tasks and more - `pip install spacy--2.2.4`\n",
        "\n",
        "`Tensorflow:`For high performance machine learning numerical computation - `pip install tensorflow--2.2.0`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r-6p508-hg7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5711ed63-c779-43f2-bc2b-8e50458db7ac"
      },
      "source": [
        "# Install relevant external libraries (Remove '#' below to install packages you may not have)\n",
        "!pip install comet_ml\n",
        "!pip install emoji\n",
        "!pip install wordcloud\n",
        "!pip install textblob\n",
        "!pip install nltk\n",
        "!pip install spacy\n",
        "!pip install tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: comet_ml in /usr/local/lib/python3.6/dist-packages (3.1.12)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.6.0)\n",
            "Requirement already satisfied: netifaces>=0.10.7 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.10.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from comet_ml) (1.12.0)\n",
            "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (7.352.0)\n",
            "Requirement already satisfied: comet-git-pure>=0.19.11 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.19.16)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.0.0)\n",
            "Requirement already satisfied: websocket-client>=0.55.0 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (0.57.0)\n",
            "Requirement already satisfied: everett[ini]>=1.0.1; python_version >= \"3.0\" in /usr/local/lib/python3.6/dist-packages (from comet_ml) (1.0.2)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from comet_ml) (2.23.0)\n",
            "Requirement already satisfied: urllib3>=1.24.1 in /usr/local/lib/python3.6/dist-packages (from comet-git-pure>=0.19.11->comet_ml) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from comet-git-pure>=0.19.11->comet_ml) (2020.6.20)\n",
            "Requirement already satisfied: configobj; extra == \"ini\" in /usr/local/lib/python3.6/dist-packages (from everett[ini]>=1.0.1; python_version >= \"3.0\"->comet_ml) (5.0.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->comet_ml) (3.0.4)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud) (7.0.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from wordcloud) (1.18.5)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.6/dist-packages (0.15.3)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.12.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (47.3.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.6.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.30.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow) (47.3.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0.post3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.1.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9F3gLo_Kke0b",
        "colab": {}
      },
      "source": [
        "# Import packages needed to solve the problem\n",
        "\n",
        "# Data analysis and wrangling\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# General natural language and text processing tools\n",
        "import re\n",
        "import string\n",
        "import emoji\n",
        "import gensim\n",
        "import spacy\n",
        "import sklearn.feature_extraction.text\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import defaultdict\n",
        "from collections import  Counter\n",
        "\n",
        "# Enhanced natural language and text processing tools\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# Processing data to prior to trainig and fitting to model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Stemming/lemmatizing/vectorizing\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Data visualisation\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "# Set plot style for data visualisation\n",
        "sns.set()\n",
        "# for improved aesthetics\n",
        "plt.style.use('ggplot')    \n",
        "\n",
        "# Importing wordcloud for plotting word clouds and \n",
        "from wordcloud import WordCloud\n",
        "# textwrap for wrapping longer text\n",
        "from textwrap import wrap\n",
        "\n",
        "# Machine Learning model versioning\n",
        "from comet_ml import Experiment\n",
        "\n",
        "# Machine learning model evaluation\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Ignore warnings because they are annoying\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vYnfg2-A9TII"
      },
      "source": [
        "### 2.2 Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vl9GIZCoGM1F",
        "colab": {}
      },
      "source": [
        "# Load train data\n",
        "train_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Y7rqWgIlPXq",
        "colab": {}
      },
      "source": [
        "# Load test data\n",
        "test_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uwmD9v11lPXw",
        "colab": {}
      },
      "source": [
        "# Load sample data\n",
        "sample_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/sample_submission.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rSbjmyfQGM1i"
      },
      "source": [
        "###### **Task: View sample of Train and Test data**\n",
        "Simply displaying the train and test data to have a look at the actual data contained within the first 10 columns.\n",
        "\n",
        "The sentiment column contains differnet classes ranging between 2 and -1 of how people view Climate Change. \n",
        "2 - New: Tweets about Factual News about climate change\n",
        "1 - Pro: Tweets that support the belief of man-made climate change\n",
        "0 - Neutral: Tweets that neither supports nor refuses the belief of of man-made climate change\n",
        "1 - Anti- Tweets that do not belief in man-made climate change\n",
        "\n",
        "The message column contains Tweets from different people expressing their views on climate change.\n",
        "\n",
        "The Tweet id column contains different  peoples Twitter accounts tweeting about climate change."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4PMfA1FOGM1i",
        "colab": {}
      },
      "source": [
        "# Print train_data\n",
        "print('Train data rows and columns:')\n",
        "train_data.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ddd8_l59GM1o",
        "colab": {}
      },
      "source": [
        "# Print test_data\n",
        "print('Test data rows and columns:')\n",
        "test_data.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UBXHM2u2GM1P"
      },
      "source": [
        "###### **Task: Display the shape of Train, Test data and Sample data**\n",
        "The `.shape` method is called to confirm the number of Rows and Columns in the DataFrame of the Train data, Test data and Sample data respectively. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsnTLYIVGM1Q",
        "colab": {}
      },
      "source": [
        "print('Train data rows and columns:', train_data.shape)\n",
        "print('Test data rows and columns:', test_data.shape)\n",
        "print('Sample data rows and columns:', sample_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YTkU_GH9GM1U"
      },
      "source": [
        "###### **Task: Investigate the Label column.**\n",
        "After loading and viewing the shape of the data, we now take a cursory view of the target variable that we ultimately need to predict. In our Initial Data Exploration, we will take a brief look at the proportionality of data to see whether it is balanced or not. In the real world, text data is rarely balanced so we expect to see imbalanced proportionality between the four class labels associated with the target variable that we aim to predict. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwHARLq3GTZ4",
        "colab_type": "text"
      },
      "source": [
        "The `.unique` method of is called on to confirm the unique values in the list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b5f_ZH8KlPYW",
        "colab": {}
      },
      "source": [
        "# View all the class labels for the target variable\n",
        "train_data['sentiment'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvN8m3fRG7BC",
        "colab_type": "text"
      },
      "source": [
        "The `.value_counts` methd is called to confirm the object containing counts of unique values. The resulting object will be in descending order so that the first element is the most frequently-occurring element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SnRzjZvylPYf",
        "colab": {}
      },
      "source": [
        "# View the breakdown of the different class labels\n",
        "train_data['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sJldCV1e9caz"
      },
      "source": [
        "# 3. Initial Data Exporation Analysis\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9L9g74L1GM2j"
      },
      "source": [
        "###### **Task: Check the proportionality of the class labels to see if data is indeed imbalanced.**\n",
        "\n",
        "Check class label proportion for:\n",
        "\n",
        "* **Anti:** the tweet does not believe in man-made climate change (**class =  -1**)\n",
        "\n",
        "* **Neutral:** the tweet neither supports nor refutes the belief of man-mad climate change (**class =  0**)\n",
        "\n",
        "* **Pro:** the tweet supports the belief of man-made climate change (**class = 1**)\n",
        "\n",
        "* **News:** the tweet links to factual news about climate change (**class = 2**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H1KJOfkSGM2k",
        "colab": {}
      },
      "source": [
        "# Calculate class label proportions\n",
        "anti_class_proportion = len(train_data.loc[train_data['sentiment']== -1]) / len(train_data)\n",
        "neut_class_proportion = len(train_data.loc[train_data['sentiment']==0]) / len(train_data)\n",
        "pro_class_proportion = len(train_data.loc[train_data['sentiment']==1]) / len(train_data)\n",
        "news_class_proportion = len(train_data.loc[train_data['sentiment']==2]) / len(train_data)\n",
        "\n",
        "# View class label proportions\n",
        "\n",
        "print(\"Anti class proportion: -1 =\", round((anti_class_proportion),2))\n",
        "print(\"Neutral class proportion: 0 =\",round((neut_class_proportion),2))\n",
        "print(\"Pro class proportion: 1 =\",round((pro_class_proportion),2))\n",
        "print(\"News class proportion: 2 =\",round((news_class_proportion),2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "khOQ1I3vlPZA"
      },
      "source": [
        "###### **Task: Check that the sum of all class label proportions are equal to 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YwF0t6FPlPZB",
        "colab": {}
      },
      "source": [
        "# Calculate and confirm that the above proportions are in fact equal to 1 (or 100%)\n",
        "total_class = anti_class_proportion + neut_class_proportion + pro_class_proportion + news_class_proportion\n",
        "total_class == 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uVoFPV3HzZs_",
        "colab": {}
      },
      "source": [
        "# Set figure size for distribution of class imbalance\n",
        "fig, ax = plt.subplots(figsize = (10, 5))\n",
        "\n",
        "# Create ditribution bar graph \n",
        "graph = sns.countplot(x = 'sentiment', data = train_data)\n",
        "\n",
        "# Give title and plot\n",
        "plt.title('Distribution of Classification Groups')\n",
        "plt.xlabel('Sentiment class labels')\n",
        "plt.ylabel('Number of tweets')\n",
        "plt.show(graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zZCEZAppARVp",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# To view proportions of the class labels, it is best practice to use pie charts\n",
        "# Where the slices will be ordered and plotted counter-clockwise:\n",
        "labels = 'Anti', 'Neutral', 'Pro', 'News'\n",
        "sizes = [anti_class_proportion, neut_class_proportion, pro_class_proportion, news_class_proportion]\n",
        "explode = (0, 0, 0.1, 0)  # Only \"explore\" the 3rd slice (i.e. 'Anti')\n",
        "\n",
        "# Create pie chart with the above labels and calculated class proportions as inputs\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
        "        shadow=True, startangle=90)\n",
        "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zNzczVZKzkiW"
      },
      "source": [
        "**Summary of Findings**\n",
        "\n",
        "After having ivestigated and seen the findings. We can confidently confirm that there is indeed an imbalance of our class labels. \n",
        "\n",
        "We can therefore deduce that Label 1 has the highest proportion of 0.58, which suggests that there are more people(58%) who tweet about supporting the belief of man-made climate change.\n",
        "\n",
        "Label 2 comes second highest proportion of 0.23 which suggests that there are 23% of people who tweet links to factual news about climate change.\n",
        "\n",
        "Label 0 comes third with a proprtion of 0.15, which suggests that there are 15% of people who tweet about neither nor refuting the belief of man-mad climate change.\n",
        "\n",
        "Lastly, there is Label -1 with the lowest proportion of 0.08, which suggests that there are 8% of people who tweet about not believing in man-made climate change."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7FSXyjWv6U_-"
      },
      "source": [
        "###3.1 Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UOEqzKnNGM2Z"
      },
      "source": [
        "**Missing values**\n",
        "\n",
        "Missing values are a common attribute in datasets and for a number of different reasons. In this part of the notebook we will do a minimal assessment of missing values. It is important however, that we understand missingness from both the perspective of the Train dayta and Test data. As such, we want to see which columns have missing data in both the Train and Test datasets, as well as the proportion of missingness in each of those columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rxm0l8PxGM2a"
      },
      "source": [
        "###### **Task: Check for missing values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_psVSAFBPYh7",
        "colab_type": "text"
      },
      "source": [
        "The `.isnull` method confirms the number of nulls that can be found in the Train dataset as the `.sum` method confirms the sum of those missing values in that Train dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XwADQdKTGM2a",
        "colab": {}
      },
      "source": [
        "# View missing values for train data\n",
        "train_data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hu5C-7ehGM2f"
      },
      "source": [
        "###### **Task: Check for whitespace strings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhP8fQ1PQKgZ",
        "colab_type": "text"
      },
      "source": [
        "The `.isspace` function confirms the existance of whitespace which  will give the characters space, tab, linefeed, return, formfeed, and vertical tab. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cxVU6lryGM2g",
        "colab": {}
      },
      "source": [
        "blanks = []  # start with an empty list\n",
        "\n",
        "for i,lb,msg,tid in train_data.itertuples():  # iterate over the DataFrame\n",
        "    if type(msg)==str:            # avoid NaN values\n",
        "        if msg.isspace():         # test 'review' for whitespace\n",
        "            blanks.append(i)     # add matching index numbers to the list\n",
        "        \n",
        "print(len(blanks), 'blanks: ', blanks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f9uEPM-PlPZa"
      },
      "source": [
        "###### **task: Check for possible duplicate tweets/retweets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lwVFt0PilPZb"
      },
      "source": [
        "<img data-attachment-id=\"1700\" data-permalink=\"https://tomraftery.com/2011/06/01/my-twitter-magic-number-is-16-whats-yours/screen-shot-2011-06-01-at-20-25-36/\" data-orig-file=\"https://i2.wp.com/tomraftery.com/wp-content/uploads/2011/06/screen-shot-2011-06-01-at-20-25-36.png?fit=563%2C271&amp;ssl=1\" data-orig-size=\"563,271\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}\" data-image-title=\"Twitter post\" data-image-description=\"<p>Twitter post staying under my Twitter magic number</p>\n",
        "\" data-medium-file=\"https://i2.wp.com/tomraftery.com/wp-content/uploads/2011/06/screen-shot-2011-06-01-at-20-25-36.png?fit=300%2C144&amp;ssl=1\" data-large-file=\"https://i2.wp.com/tomraftery.com/wp-content/uploads/2011/06/screen-shot-2011-06-01-at-20-25-36.png?fit=563%2C271&amp;ssl=1\" class=\"size-full wp-image-1700 jetpack-lazy-image jetpack-lazy-image--handled\" title=\"Twitter post\" src=\"https://i0.wp.com/www.enterpriseirregulars.com/wp-content/uploads/2011/06/a06675977df79f3e834fdf758225008f1.png?resize=563%2C271&amp;ssl=1\" alt=\"Twitter post\" width=\"563\" height=\"271\" data-recalc-dims=\"1\" data-lazy-loaded=\"1\">\n",
        "\n",
        "<p>Twitter is a superb medium for getting a message out.</p>\n",
        "\n",
        "<p>And it’s RT (Retweet) convention means that tweets can go viral very quickly. So this may give us an indication of virality and sentiment for certain classes of tweets as there are a number of RT's visible in our data. But how many are there? and for which sentiment classes?</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pawf4ezIjp8f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check for duplicate messages/tweets in the train data\n",
        "dups_train = train_data['message']\n",
        "dups_train = train_data[dups_train.isin(dups_train[dups_train.duplicated()])].sort_values(\"message\")\n",
        "# Check for duplicate tweet ID's in the train data to validate that each message is unique\n",
        "dups_tweet_tr = train_data['tweetid']\n",
        "train_data[dups_tweet_tr.isin(dups_tweet_tr[dups_tweet_tr.duplicated()])].sort_values(\"message\")\n",
        "# Check for duplicate messages/tweets in the test data\n",
        "dups_test = test_data['message']\n",
        "dups_test = test_data[dups_test.isin(dups_test[dups_test.duplicated()])].sort_values(\"message\")\n",
        "# Check for duplicate tweet ID's in the test data to validate that each message is unique\n",
        "dups_tweet_te = train_data['tweetid']\n",
        "train_data[dups_tweet_te.isin(dups_tweet_te[dups_tweet_te.duplicated()])].sort_values(\"message\")\n",
        "\n",
        "print('Duplicate tweet messages in train data rows and columns:',dups_train.shape)\n",
        "print('Duplicate tweet messages in test data rows and columns:',dups_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zuFJIDzalPZs",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "#Confirm whether sample of duplicates in train data is generally comprised of retweets\n",
        "dups_train.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AqTM3d66lPaC",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "#Confirm whether sample of duplicates in test data is generally comprised of retweets\n",
        "dups_test.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6w134PL5lPaQ",
        "colab": {}
      },
      "source": [
        "# View the differences in proportions of duplicates across train and test datasets\n",
        "dups_train_prop = len(dups_train)/len(train_data['message'])\n",
        "dups_test_prop = len(dups_test)/len(test_data['message'])\n",
        "print('Train data proportion of duplicates/RTs:',round((dups_train_prop),2))\n",
        "print('Test data proportion of duplicates/RTs:',round((dups_test_prop),2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zr_MrlKZlPaV"
      },
      "source": [
        "After having explored the data, the findings deduce that 12% of the train data contains duplicates and 11% of our test data contains duplicates. \n",
        "The proportion of duplicate messages across both data sets is similar. Additionally, a retweet seem to almost always be associated with an '@' mention in a message.\n",
        "\n",
        " For more on twitter terminology, please visit the Twitter help page [here](https://help.twitter.com/en/using-twitter/types-of-tweets).\n",
        "This might be useful information when deciding which feeatures to generate and select for our final model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Af_xQ6CHObNp"
      },
      "source": [
        "**Summary of findings**\n",
        "\n",
        "**Exploratory Data Analysis (EDA)** refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations\"). Though we have not delved very deeply into EDA, we have mangaged to capture a cursory view into our data as a precursor to the EDA that will follow on our train data.\n",
        "\n",
        "After performing this initial data exploration on the given data the following observations were made:\n",
        " * 1 (**Pro**) = 8530 (**53.9%**)\n",
        " * 2 (**News**) = 3640 (**23%**)\n",
        " * 0 (**Neutral**) = 2353 (**14.9%**)\n",
        " * -1 (**Anti**) = 1296 (**8.2%**)\n",
        "\n",
        "This suggests that a majority (over 50%) of the tweets support the belief in man-made climate change, while 20% were Neutral about this topic.\n",
        " We can assume(hypothesize) that a positive change can be made by driving the right campaigns and marketing messaging on different marketing channels that would be applicable for a Non-Profit Organisations (NPO) specialising in climate change as there are more Pro sentiments than Anti sentiments that man-made climate change exists. \n",
        "\n",
        "A business looking to launch a new product\\service where environmentally friendly products are valued by their clients would also benefit from this information as an input into their market research. Sentiment analysis from text classification may prove to be quicker and/or more cost effective than the traditional approach of doing surveys or paying for survey data to inform an organisation's qualitative market research efforts. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjWRZIWSSWnb",
        "colab_type": "text"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UXuEt0qblPaY"
      },
      "source": [
        "In the next several tasks, a first iteration of text cleaning taking a very manual text preprocessing approach to text cleaning which works but is intentionally imperfect in our first iteration will be explored.\n",
        "\n",
        "The reason for this is to show that text cleaning in NLP can be an iterative process which is defined by the context of data you need for the particular machine learning problem you are solving for. Here, a step by step approach of some of the text cleaning tools available to us will be shown. \n",
        "\n",
        "Further down in the notebook, a more simplified approach will be taken once we understand the steps involved and how each step modifies our data so that it is ready for some more data exploration and is useable for our prediction model for this particular problem which relates to multiclass classification within the range of those who believe and don't believe that climate change exists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nKUk8-LnlPaf"
      },
      "source": [
        "###### **Task: Review sample of uncleaned tweets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nfq36sqqT2sC",
        "colab_type": "text"
      },
      "source": [
        "Again, the `.unique` method confirms only the unique messages in the list/data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pF_MqYhvlPaf",
        "colab": {}
      },
      "source": [
        "# View sample tweet messages\n",
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OqR7EPr2lPak",
        "colab": {}
      },
      "source": [
        "# Try removing non-ASCII strings as a start e.g. byÃ¢â‚¬Â¦\n",
        "train_data['message']=train_data['message'].apply(lambda x: x.split('Ã¢â‚¬Â¦')[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iJdcYJualPao",
        "colab": {}
      },
      "source": [
        "# View change to sample tweet messages\n",
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "biDGHQ9AlPav",
        "colab": {}
      },
      "source": [
        "# Remove URL's\n",
        "train_data['message']=train_data['message'].apply(lambda x: re.sub(r\"\\bhttps://t.co/\\w+\", '', x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YS9ybyoLlPaz",
        "colab": {}
      },
      "source": [
        "# View change to sample tweet messages\n",
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tw0H_b7glPa6",
        "colab": {}
      },
      "source": [
        "# Remove Line breaks\n",
        "train_data['message']=train_data['message'].replace('\\n', ' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E9Tq01DolPa_",
        "colab": {}
      },
      "source": [
        "# View change to sample tweet messages\n",
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1kQtS1m1lPbF",
        "colab": {}
      },
      "source": [
        "# Remove numbers\n",
        "train_data['message']=train_data['message'].apply(lambda x: re.sub('\\w*\\d\\w*', ' ', x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZoZ5LCcllPbJ",
        "colab": {}
      },
      "source": [
        "# View change to sample tweet messages\n",
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U8MUmds8lPbM",
        "colab": {}
      },
      "source": [
        "# Remove capital letters and punctuations\n",
        "\n",
        "train_data['message']=train_data['message'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R-EbhW_PlPbQ",
        "colab": {}
      },
      "source": [
        "# View change to sample tweet messages\n",
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QDwpHRznlPbV",
        "colab": {}
      },
      "source": [
        "# Dictionary of English Contractions\n",
        "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
        "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\", \"it's\": \"it is\" ,\n",
        "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
        "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
        "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
        "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
        "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
        "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
        "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
        "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
        "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
        "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
        "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
        "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
        "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
        "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
        "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
        "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
        "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
        "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
        "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
        "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
        "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
        "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
        "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
        "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
        "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
        "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
        "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
        "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
        "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
        "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
        "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
        "                     \"you've\": \"you have\", \"we’ ve\": \"we have\", \"imvotingbecause\": \"i am voting because\", \"rt\": \"\" }\n",
        "\n",
        "# Regular expression for finding contractions\n",
        "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "# Function for expanding contractions\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "  def replace(match):\n",
        "    return contractions_dict[match.group(0)]\n",
        "  return contractions_re.sub(replace, text)\n",
        "\n",
        "# Expanding Contractions in the reviews\n",
        "train_data['message']=train_data['message'].apply(lambda x:expand_contractions(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p71-AMHTlPbY",
        "colab": {}
      },
      "source": [
        "# View change to sample tweet messages\n",
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e2oBIxYilPbc",
        "colab": {}
      },
      "source": [
        "# Convert capital letters to lowercase\n",
        "train_data['message']=train_data['message'].apply(lambda x: x.lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YUVXTliOlPbh",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# View change to sample tweet messages\n",
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IRWZvtSLlPbl",
        "colab": {}
      },
      "source": [
        "# Remove digits and words containing digits\n",
        "train_data['message']=train_data['message'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m42Tj20ylPbr",
        "colab": {}
      },
      "source": [
        "# View change to sample tweet messages\n",
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fetOHgEalPbv",
        "colab": {}
      },
      "source": [
        "# Remove punctuation\n",
        "\n",
        "train_data['message']=train_data['message'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hx0ujRAOlPb1",
        "colab": {}
      },
      "source": [
        "# View change to sample tweet messages\n",
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Om5VFsVKlPb5",
        "colab": {}
      },
      "source": [
        "# Remove extra spaces\n",
        "train_data['message']=train_data['message'].apply(lambda x: re.sub(' +',' ',x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JLvQX2rplPb9",
        "colab": {}
      },
      "source": [
        "# View change to sample tweet messages\n",
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g20omEG9lPcC",
        "colab": {}
      },
      "source": [
        "# View changes to train_data\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9ZvvFOV9lPcI",
        "colab": {}
      },
      "source": [
        "# Function to remove emojis\n",
        "#!pip install emoji\n",
        "#import emoji\n",
        "#import string\n",
        "\n",
        "def give_emoji_free_text(text):\n",
        "    \n",
        "    '''\n",
        "    Takes in tweet series, removes all emojis, and returns cleaned tweet series.\n",
        "    '''\n",
        "    \n",
        "    allchars = [str for str in text]\n",
        "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
        "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
        "\n",
        "    return clean_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GEMHAPeNlPcM",
        "colab": {}
      },
      "source": [
        "# Remove emojis\n",
        "full_text_list = []\n",
        "\n",
        "for index, rows in train_data['message'].iteritems():\n",
        "    rows = give_emoji_free_text(rows) # remove emojis\n",
        "    full_text_list.append(rows)\n",
        "    \n",
        "train_data['message'] = full_text_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TM3G9Oc4lPcQ",
        "colab": {}
      },
      "source": [
        "# View changes to train_data\n",
        "train_data['message'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TntwyQJMlPcS",
        "colab": {}
      },
      "source": [
        "# Try removing non-ASCII strings if they persist e.g. iã¢â‚¬â¦\n",
        "train_data['message']=train_data['message'].apply(lambda x: x.split('iã¢â‚¬â¦')[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E38MD179lPcX",
        "colab": {}
      },
      "source": [
        "# View sample changes of train_data tweet messages in plain text\n",
        "for index,text in enumerate(train_data['message'][35:40]):\n",
        "  print('Message %d:\\n'%(index+1),text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DPubkW_-lPch"
      },
      "source": [
        "###### **Task: Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gsZqLUOUlPch",
        "colab": {}
      },
      "source": [
        "# Loading model\n",
        "nlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ynuZg8PElPco",
        "colab": {}
      },
      "source": [
        "# Lemmatization with stopwords removal\n",
        "train_data['lemmatized']=train_data['lemmatized']=train_data['message'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))\n",
        "train_data['lemmatized']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nYY57wBOlPcr",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# View changes to train_data lemmatized (though this is not perfect at this stage)\n",
        "train_data_grouped=train_data[['message','lemmatized']].groupby(by='message').agg(lambda x:' '.join(x))\n",
        "train_data_grouped.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vg30L4PilPcw",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# View changes to train_data\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "erI5JZZmlPcy",
        "colab": {}
      },
      "source": [
        "# View sample of lemmatized changes to train_data\n",
        "train_data['lemmatized'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ytB6qkkqlPc1",
        "colab": {}
      },
      "source": [
        "# View sample of lemmatized changes of train_data tweet messages in plain text\n",
        "for index,text in enumerate(train_data['lemmatized'][35:40]):\n",
        "  print('Lemmatized Message %d:\\n'%(index+1),text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IuwZ-qcmlPc3"
      },
      "source": [
        "###### **Task: Document Term Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4LHZl5ULlPc4",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Creating Document Term Matrix\n",
        "token = RegexpTokenizer(r'[@a-zA-Z0-9]+')\n",
        "cv=CountVectorizer(analyzer='word',lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\n",
        "text_counts=cv.fit_transform(train_data['lemmatized'])\n",
        "df_dtm = pd.DataFrame(text_counts.toarray(), columns=cv.get_feature_names())\n",
        "df_dtm.index=train_data['lemmatized'].index\n",
        "df_dtm.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZKLzTLPOADZ",
        "colab_type": "text"
      },
      "source": [
        " #### NDU PLEASE GIVE US Summary of findings FOR THIS TEXT CLEANING PROCESS\n",
        " xxxxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D7zWIFrXlPdA"
      },
      "source": [
        "# 4. Further Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m6LSV9GqlPdA",
        "colab": {}
      },
      "source": [
        "# Load train data\n",
        "train_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O0dohfbilPdD",
        "colab": {}
      },
      "source": [
        "# Load test data\n",
        "test_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/test.csv')\n",
        "print('Test data rows and columns:')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GqDBwDc2lPdF",
        "colab": {}
      },
      "source": [
        "# Load sample data\n",
        "sample_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/sample_submission.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "91ue8sWm_BWc"
      },
      "source": [
        "**Task: Visualize current state of data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_iGCdfQflPdR",
        "colab": {}
      },
      "source": [
        "def get_top_tweet_unigrams(corpus, n=None):\n",
        "    '''\n",
        "    Function returns a unigram \n",
        "\n",
        "    '''\n",
        "\n",
        "    vec = CountVectorizer(ngram_range=(1, 1)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZCdWoLGblPdU",
        "colab": {}
      },
      "source": [
        "#Initial minimally cleaned text for top 20 unigram count\n",
        "plt.figure(figsize=(10,5))\n",
        "top_tweet_unigrams = get_top_tweet_unigrams(train_data['message'])[:20]\n",
        "x,y = map(list,zip(*top_tweet_unigrams))\n",
        "sns.barplot(x=y, y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK9GvpVGPPbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_top_tweet_bigrams(corpus, n=None):\n",
        "    '''\n",
        "    Function returns a biigram \n",
        "    '''\n",
        "    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dLHa2DPmlPdW",
        "colab": {}
      },
      "source": [
        "# Initial minimally cleaned text for top 20 bigram count\n",
        "plt.figure(figsize=(10,5))\n",
        "top_tweet_bigrams=get_top_tweet_bigrams(train_data['message'])[:20]\n",
        "x,y=map(list,zip(*top_tweet_bigrams))\n",
        "sns.barplot(x=y,y=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tF7S_xfQJvNl"
      },
      "source": [
        "## COMMENT ON THE DIAGRAM BEFORE THE CLUSTERING. GIVE DETAILED STORY \n",
        "## NDU AND VICKY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MAo_9M_5lPdb",
        "colab": {}
      },
      "source": [
        "# Create column for the number of words in tweet\n",
        "train_data['word_count'] = train_data['message'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Split so we can use updated train set with new feature\n",
        "train_data = train_data[:len(train_data)]\n",
        "\n",
        "# Define subplot to see graphs side by side\n",
        "fig, ax = plt.subplots(figsize = (10, 5))\n",
        "\n",
        "#create graphs\n",
        "sns.kdeplot(train_data['word_count'][train_data['sentiment'] == 0], shade = True, label = 'Neutral')\n",
        "sns.kdeplot(train_data['word_count'][train_data['sentiment'] == 1], shade = True, label = 'Pro')\n",
        "sns.kdeplot(train_data['word_count'][train_data['sentiment'] == 2], shade = True, label = 'News')\n",
        "sns.kdeplot(train_data['word_count'][train_data['sentiment'] == -1], shade = True, label = 'Anti')\n",
        "\n",
        "# Set title and plot\n",
        "plt.title('Distribution of Tweet Word Count')\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Sentiments Proportions')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DW5IS1ahlPdd",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Create column for the number of characters in a tweet\n",
        "train_data['character_count'] = train_data['message'].apply(lambda x: len(x))\n",
        "\n",
        "# Split so we can use updated train set with new feature\n",
        "train_data = train_data[:len(train_data)]\n",
        "\n",
        "# Define subplot to see graphs side by side\n",
        "fig, ax = plt.subplots(figsize = (10, 5))\n",
        "\n",
        "# Create graphs\n",
        "sns.kdeplot(train_data['character_count'][train_data['sentiment'] == 0], shade = True, label = 'Neutral')\n",
        "sns.kdeplot(train_data['character_count'][train_data['sentiment'] == 1], shade = True, label = 'Pro')\n",
        "sns.kdeplot(train_data['character_count'][train_data['sentiment'] == 2], shade = True, label = 'News')\n",
        "sns.kdeplot(train_data['character_count'][train_data['sentiment'] == -1], shade = True, label = 'Anti')\n",
        "\n",
        "\n",
        "# Set title and plot\n",
        "plt.title('Distribution of Tweet Character Count')\n",
        "plt.xlabel('Character Count')\n",
        "plt.ylabel('Sentiment Probability')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AV8g9fpKlPdg",
        "colab": {}
      },
      "source": [
        "def average_word_length(x):\n",
        "    '''\n",
        "    #Function to find average word length \n",
        "    '''\n",
        "    x = x.split()\n",
        "    return np.mean([len(i) for i in x])\n",
        "\n",
        "# Broadcast to text column\n",
        "train_data['average_word_length'] = train_data['message'].apply(average_word_length)\n",
        "\n",
        "# Split so we can use updated train set with new feature\n",
        "train_data = train_data[:len(train_data)]\n",
        "\n",
        "# Define subplot to see graphs side by side\n",
        "fig, ax = plt.subplots(figsize = (10, 5))\n",
        "\n",
        "# Create graphs\n",
        "sns.kdeplot(train_data['average_word_length'][train_data['sentiment'] == 0], shade = True, label = 'Neutral')\n",
        "sns.kdeplot(train_data['average_word_length'][train_data['sentiment'] == 1], shade = True, label = 'Pro')\n",
        "sns.kdeplot(train_data['average_word_length'][train_data['sentiment'] == 2], shade = True, label = 'News')\n",
        "sns.kdeplot(train_data['average_word_length'][train_data['sentiment'] == -1], shade = True, label = 'Anti')\n",
        "\n",
        "# Set title\n",
        "plt.title('Distribution of Tweet Average Word Length')\n",
        "plt.xlabel('Average Word Length')\n",
        "plt.ylabel('Sentiment Probability')\n",
        "\n",
        "# Plot graphs\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YPPlB_8plPdl"
      },
      "source": [
        "xxxxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "06jnwSNplPdm"
      },
      "source": [
        "## 4.1 Insights    ##NDUDUZO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j65NzMnaGM2p"
      },
      "source": [
        "##### Analysing the  `label` column in relation to text data\n",
        "xxxxxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "heading_collapsed": true,
        "id": "7cIJ8jmylPdo"
      },
      "source": [
        "##### Conclude\n",
        "By now, we should be so much more familiar with the data. Let's go a little further with Exploratory Data Analysis (EDA) withing our initial Feature Engineering/Extraction steps that are to follow. Now that an idea of what new features to construct has been identfied, and how they might be useful, let's add the rest of them and visualize them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bXWGq-NGlPdp"
      },
      "source": [
        "# 5. Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QF68QabKZ9q",
        "colab_type": "text"
      },
      "source": [
        "Just a brief definition of what Feature engineering is so that there is an understanding on what this section focuses on:\n",
        "\n",
        "Feature engineering is the process of using domain knowledge to extract features from raw data via data mining techniques. These features can be used to improve the performance of machine learning algorithms. Feature engineering can be considered as applied machine learning itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d25J72dKlPdq"
      },
      "source": [
        "###### **Task: Generate New Features and Visualise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tNGBHLPAlPds",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "#add unique word count\n",
        "train_data['unique_word_count'] = train_data['message'].apply(lambda x: len(set(x.split())))\n",
        "\n",
        "#add stopword count\n",
        "stopwords = stopwords.words('english')\n",
        "train_data['stopword_count'] = train_data['message'].apply(lambda x: len([i for i in x.lower().split() if i in stopwords]))\n",
        "\n",
        "#add url count\n",
        "train_data['url_count'] = train_data['message'].apply(lambda x: len([i for i in x.lower().split() if 'http' in i or 'https' in i]))\n",
        "\n",
        "# add hashtag_count\n",
        "train_data['hashtag_count'] = train_data['message'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
        "\n",
        "#add mention count\n",
        "train_data['mention_count'] = train_data['message'].apply(lambda x: len([i for i in str(x) if i == '@']))\n",
        "\n",
        "#add punctuation count\n",
        "train_data['punctuation_count'] = train_data['message'].apply(lambda x: len([i for i in str(x) if i in string.punctuation]))\n",
        "\n",
        "#split so we can use updated train set\n",
        "train_data = train_data[:len(train_data)]\n",
        "\n",
        "disaster = train_data['sentiment'].astype(int) == 1\n",
        "\n",
        "#produce graphs to visualize newly added features\n",
        "fig, axes = plt.subplots(6, figsize=(20, 30))\n",
        "\n",
        "graph1 = sns.kdeplot(train_data.loc[~disaster]['unique_word_count'], shade = True, label = 'Neutral', ax=axes[0])\n",
        "graph1 = sns.kdeplot(train_data.loc[disaster]['unique_word_count'], shade = True, label = 'Pro', ax=axes[0])\n",
        "graph1 = sns.kdeplot(train_data.loc[disaster]['unique_word_count'], shade = True, label = 'News', ax=axes[0])\n",
        "graph1 = sns.kdeplot(train_data.loc[disaster]['unique_word_count'], shade = True, label = 'Anti', ax=axes[0])\n",
        "graph1.set_title('Distribution of Unique Word Count')\n",
        "# plt.xlabel('unique_word_count')\n",
        "\n",
        "graph2 = sns.kdeplot(train_data.loc[~disaster]['stopword_count'], shade = True, label = 'Neutral', ax=axes[1])\n",
        "graph2 = sns.kdeplot(train_data.loc[disaster]['stopword_count'], shade = True, label = 'Pro', ax=axes[1])\n",
        "graph2 = sns.kdeplot(train_data.loc[disaster]['stopword_count'], shade = True, label = 'News', ax=axes[1])\n",
        "graph2 = sns.kdeplot(train_data.loc[disaster]['stopword_count'], shade = True, label = 'Anti', ax=axes[1])\n",
        "graph2.set_title('Distribution of Stopword Word Count')\n",
        "\n",
        "graph3 = sns.kdeplot(train_data.loc[~disaster]['url_count'], shade = True, label = 'Neutral', ax=axes[2])\n",
        "graph3 = sns.kdeplot(train_data.loc[disaster]['url_count'], shade = True, label = 'Pro', ax=axes[2])\n",
        "graph3 = sns.kdeplot(train_data.loc[disaster]['url_count'], shade = True, label = 'News', ax=axes[2])\n",
        "graph3 = sns.kdeplot(train_data.loc[disaster]['url_count'], shade = True, label = 'Anti', ax=axes[2])\n",
        "graph3.set_title('Distribution of URL Count')\n",
        "\n",
        "graph4 = sns.kdeplot(train_data.loc[~disaster]['hashtag_count'], shade = True,  label = 'Neutral', ax=axes[3], bw = 1)\n",
        "graph4 = sns.kdeplot(train_data.loc[disaster]['hashtag_count'], shade = True, label = 'Pro', ax=axes[3], bw =1)\n",
        "graph4 = sns.kdeplot(train_data.loc[disaster]['hashtag_count'], shade = True, label = 'News', ax=axes[3], bw =1)\n",
        "graph4 = sns.kdeplot(train_data.loc[disaster]['hashtag_count'], shade = True, label = 'Anti', ax=axes[3], bw =1)\n",
        "graph4.set_title('Distribution of Hashtag Count')\n",
        "\n",
        "graph4 = sns.kdeplot(train_data.loc[~disaster]['mention_count'], shade = True,  label = 'Neutral', ax=axes[4], bw = 1)\n",
        "graph4 = sns.kdeplot(train_data.loc[disaster]['mention_count'], shade = True, label = 'Pro', ax=axes[4], bw =1)\n",
        "graph4 = sns.kdeplot(train_data.loc[disaster]['mention_count'], shade = True, label = 'News', ax=axes[4], bw =1)\n",
        "graph4 = sns.kdeplot(train_data.loc[disaster]['mention_count'], shade = True, label = 'Anti', ax=axes[4], bw =1)\n",
        "graph4.set_title('Distribution of Mention Count')\n",
        "\n",
        "graph5 = sns.kdeplot(train_data.loc[~disaster]['punctuation_count'], shade = True, label = 'Neutral', ax=axes[5], bw = 1)\n",
        "graph5 = sns.kdeplot(train_data.loc[disaster]['punctuation_count'], shade = True, label = 'Pro', ax=axes[5], bw = 1)\n",
        "graph5 = sns.kdeplot(train_data.loc[disaster]['punctuation_count'], shade = True, label = 'News', ax=axes[5], bw = 1)\n",
        "graph5 = sns.kdeplot(train_data.loc[disaster]['punctuation_count'], shade = True, label = 'Anti', ax=axes[5], bw = 1)\n",
        "graph5.set_title('Distribution of Punctuation Count')\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QVMn_YnBlPdv",
        "colab": {}
      },
      "source": [
        "# View first five parts of the new features\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "03JNcBRWlPd1"
      },
      "source": [
        "### 5.1 Further Pre-Processing\n",
        "* ***Data Cleaning***\n",
        "* ***Using NLP***\n",
        "\n",
        "Now that the data has been explored, it will now be prepared for machine learning. In general, to process text the following procedure needs to be explained:\n",
        "\n",
        "raw text corpus -> processing text -> tokenized text -> corpus vocabulary -> text representation\n",
        "\n",
        "Most of the hard work can be done with Keras's Tokenize object, which automatically converts all words to lowercase and filters out punctuation\n",
        "\n",
        "This tokenizer has many arguements that allow you to do most of the cleaning with one line of code, so we do not need to much processing ourselves. Examples have been included of how one would manually clean text for reference:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ybl0JhLMlPd2"
      },
      "source": [
        "### Create Preprocessing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oNCsxIaolPd2",
        "colab": {}
      },
      "source": [
        "# Remove punctuation\n",
        "def remove_punctuation(message):\n",
        "    return message.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Remove stopwords\n",
        "StopWords = stopwords\n",
        "def remove_stopwords(message):\n",
        "    return ' '.join([i for i in message.split() if i not in StopWords])\n",
        "\n",
        "# Remove words less than 4 \n",
        "def remove_less_than(message):\n",
        "    return ' '.join([i for i in message.split() if len(i) > 3])\n",
        "\n",
        "# Remove words with non-alphabet characters\n",
        "def remove_non_alphabet(message):\n",
        "    return ' '.join([i for i in message.split() if i.isalpha() == True])\n",
        "\n",
        "# Stem words\n",
        "stemmer = SnowballStemmer('english')\n",
        "def stem_words(message):\n",
        "    return stemmer.stem(message)\n",
        "\n",
        "# Lemmatize words for verb\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words_verb(message):\n",
        "    return lemmatizer.lemmatize(message, 'v')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RSebf1gClPd5",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Demonstrate the efficiency and effectiveness of Keras package\n",
        "# In contrast to RegExTokenizer from nltk libarary used in first iteration of text cleaning\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "\n",
        "contractions = [\"can't stop won't stop\"]\n",
        "\n",
        "tokenizer = Tokenizer(filters= \"'!#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\")\n",
        "tokenizer.fit_on_texts(contractions)\n",
        "tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o2K79NW5lPd9",
        "colab": {}
      },
      "source": [
        "def correct_contraction(message):\n",
        "    '''\n",
        "    Function will remove all the contractions from the text\n",
        "\n",
        "    '''\n",
        "    message = str(message).lower()\n",
        "    message = re.sub(r\"he's\", \"he is\", message)\n",
        "    message = re.sub(r\"there's\", \"there is\", message)\n",
        "    message = re.sub(r\"We're\", \"We are\", message)\n",
        "    message = re.sub(r\"That's\", \"That is\", message)\n",
        "    message = re.sub(r\"won't\", \"will not\", message)\n",
        "    message = re.sub(r\"they're\", \"they are\", message)\n",
        "    message = re.sub(r\"Can't\", \"Cannot\", message)\n",
        "    message = re.sub(r\"wasn't\", \"was not\", message)\n",
        "    message = re.sub(r\"aren't\", \"are not\", message)\n",
        "    message = re.sub(r\"isn't\", \"is not\", message)\n",
        "    message = re.sub(r\"What's\", \"What is\", message)\n",
        "    message = re.sub(r\"i'd\", \"I would\", message)\n",
        "    message = re.sub(r\"should've\", \"should have\", message)\n",
        "    message = re.sub(r\"where's\", \"where is\", message)\n",
        "    message = re.sub(r\"we'd\", \"we would\", message)\n",
        "    message = re.sub(r\"i'll\", \"I will\", message)\n",
        "    message = re.sub(r\"weren't\", \"were not\", message)\n",
        "    message = re.sub(r\"They're\", \"They are\", message)\n",
        "    message = re.sub(r\"let's\", \"let us\", message)\n",
        "    message = re.sub(r\"it's\", \"it is\", message)\n",
        "    message = re.sub(r\"can't\", \"cannot\", message)\n",
        "    message = re.sub(r\"don't\", \"do not\", message)\n",
        "    message = re.sub(r\"you're\", \"you are\", message)\n",
        "    message = re.sub(r\"i've\", \"I have\", message)\n",
        "    message = re.sub(r\"that's\", \"that is\", message)\n",
        "    message = re.sub(r\"i'll\", \"I will\", message)\n",
        "    message = re.sub(r\"doesn't\", \"does not\", message)\n",
        "    message = re.sub(r\"i'd\", \"I would\", message)\n",
        "    message = re.sub(r\"didn't\", \"did not\", message)\n",
        "    message = re.sub(r\"ain't\", \"am not\", message)\n",
        "    message = re.sub(r\"you'll\", \"you will\", message)\n",
        "    message = re.sub(r\"I've\", \"I have\", message)\n",
        "    message = re.sub(r\"Don't\", \"do not\", message)\n",
        "    message = re.sub(r\"I'll\", \"I will\", message)\n",
        "    message = re.sub(r\"I'd\", \"I would\", message)\n",
        "    message = re.sub(r\"Let's\", \"Let us\", message)\n",
        "    message = re.sub(r\"you'd\", \"You would\", message)\n",
        "    message = re.sub(r\"It's\", \"It is\", message)\n",
        "    message = re.sub(r\"Ain't\", \"am not\", message)\n",
        "    message = re.sub(r\"Haven't\", \"Have not\", message)\n",
        "    message = re.sub(r\"Could've\", \"Could have\", message)\n",
        "    message = re.sub(r\"youve\", \"you have\", message)\n",
        "    message = re.sub(r\"haven't\", \"have not\", message)\n",
        "    message = re.sub(r\"hasn't\", \"has not\", message)\n",
        "    message = re.sub(r\"There's\", \"There is\", message)\n",
        "    message = re.sub(r\"He's\", \"He is\", message)\n",
        "    message = re.sub(r\"It's\", \"It is\", message)\n",
        "    message = re.sub(r\"You're\", \"You are\", message)\n",
        "    message = re.sub(r\"I'M\", \"I am\", message)\n",
        "    message = re.sub(r\"shouldn't\", \"should not\", message)\n",
        "    message = re.sub(r\"wouldn't\", \"would not\", message)\n",
        "    message = re.sub(r\"i'm\", \"I am\", message)\n",
        "    message = re.sub(r\"I'm\", \"I am\", message)\n",
        "    message = re.sub(r\"Isn't\", \"is not\", message)\n",
        "    message = re.sub(r\"Here's\", \"Here is\", message)\n",
        "    message = re.sub(r\"you've\", \"you have\", message)\n",
        "    message = re.sub(r\"we're\", \"we are\", message)\n",
        "    message = re.sub(r\"what's\", \"what is\", message)\n",
        "    message = re.sub(r\"couldn't\", \"could not\", message)\n",
        "    message = re.sub(r\"we've\", \"we have\", message)\n",
        "    message = re.sub(r\"who's\", \"who is\", message)\n",
        "    message = re.sub(r\"y'all\", \"you all\", message)\n",
        "    message = re.sub(r\"would've\", \"would have\", message)\n",
        "    message = re.sub(r\"it'll\", \"it will\", message)\n",
        "    message = re.sub(r\"we'll\", \"we will\", message)\n",
        "    message = re.sub(r\"We've\", \"We have\", message)\n",
        "    message = re.sub(r\"he'll\", \"he will\", message)\n",
        "    message = re.sub(r\"Y'all\", \"You all\", message)\n",
        "    message = re.sub(r\"Weren't\", \"Were not\", message)\n",
        "    message = re.sub(r\"Didn't\", \"Did not\", message)\n",
        "    message = re.sub(r\"they'll\", \"they will\", message)\n",
        "    message = re.sub(r\"they'd\", \"they would\", message)\n",
        "    message = re.sub(r\"DON'T\", \"DO NOT\", message)\n",
        "    message = re.sub(r\"they've\", \"they have\", message)\n",
        "    \n",
        "    #correct some acronyms while we are at it\n",
        "    message = re.sub(r\"nba\", \"National Basketball Association\", message)\n",
        "    message = re.sub(r\"azwx\", \"Arizona Weather\", message)  \n",
        "    message = re.sub(r\"alwx\", \"Alabama Weather\", message)\n",
        "    message = re.sub(r\"wordpressdotcom\", \"wordpress\", message)      \n",
        "    message = re.sub(r\"gawx\", \"Georgia Weather\", message)  \n",
        "    message = re.sub(r\"scwx\", \"South Carolina Weather\", message)  \n",
        "    message = re.sub(r\"cawx\", \"California Weather\", message)\n",
        "    message = re.sub(r\"usNWSgov\", \"United States National Weather Service\", message) \n",
        "    message = re.sub(r\"epa\", \"Environmental Protection Agency\", message)\n",
        "    message = re.sub(r\"okwx\", \"Oklahoma City Weather\", message)\n",
        "    message = re.sub(r\"rt\", \"retweet\", message)\n",
        "    message = re.sub(r\"ny\", \"New York\", message)\n",
        "    message = re.sub(r\"arwx\", \"Arkansas Weather\", message)  \n",
        "  \n",
        "    \n",
        "    return message\n",
        "\n",
        "train_data['message'] = train_data['message'].apply(correct_contraction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mjcvRxW6lPeB",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# View the fist five changes to train data\n",
        "train_data['message'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qNfV7E1olPeD"
      },
      "source": [
        "### Remove URL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgOedHeJ1XMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " example = \"My Profile: https://auth.geeksforgeeks.org\\\n",
        "    / user / Chinmoy % 20Lenka / articles in\\\n",
        "    the portal of http://www.geeksforgeeks.org/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6B5hcjL7lPeF",
        "colab": {}
      },
      "source": [
        "def remove_URL(message):\n",
        "    '''\n",
        "    This funtion will remove all the URL's that exist in the texts\n",
        "\n",
        "    '''\n",
        "    \n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return url.sub(r'',message)\n",
        "\n",
        "remove_URL(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BC_aoIrslPeH",
        "colab": {}
      },
      "source": [
        "# Apply function to remove URL\n",
        "train_data['message']=train_data['message'].apply(lambda x : remove_URL(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VoDXpVtrlPeJ"
      },
      "source": [
        "### Remove line breaks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtWRlqCG1e5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example = \"Hello \\n World!\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_TkPyQQlPeO",
        "colab": {}
      },
      "source": [
        "def line_break(message):\n",
        "\n",
        "  '''\n",
        "  Function will remove all the line breaks that exist in the texts\n",
        "  \n",
        "  '''\n",
        "  \n",
        "  break_line = message.replace('\\n', ' ')\n",
        "  return break_line\n",
        "\n",
        "line_break(example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vqFeADpXlPeQ",
        "colab": {}
      },
      "source": [
        "# Apply function to remove line breaks\n",
        "train_data['message']=train_data['message'].apply(lambda x : line_break(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YzQNLhGElPeS"
      },
      "source": [
        "### Remove HTML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_m5ZJMh0_Na",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example = \"\"\"<div>\n",
        "<h1>Reel or Real</h1>\n",
        "<p>Hindustan </p>\n",
        "<a href=\"https://www.hindustan.com/c/nlp-open-the source\">get the source</a>\n",
        "</div>\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h4g1eZAYlPeV",
        "colab": {}
      },
      "source": [
        "# Function to remove HTML tags\n",
        "def remove_html(message):\n",
        "    '''\n",
        "    Function will remove all the HTML tags that exist in the texts\n",
        "\n",
        "    '''\n",
        "\n",
        "    html=re.compile(r'<.*?>')\n",
        "    return html.sub(r'',message)\n",
        "print(remove_html(example))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XCP4ORavlPea",
        "colab": {}
      },
      "source": [
        "# Function to remove HTML tags\n",
        "train_data['message']=train_data['message'].apply(lambda x : remove_html(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K-qw4_uOlPec"
      },
      "source": [
        "### Remove emojis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pQrJ-xP2lPec",
        "colab": {}
      },
      "source": [
        "# Function to remove emojis in Unicode\n",
        "def remove_emoji(message):\n",
        "    '''\n",
        "    Function will remove all the emojis in unicodes that exist in the texts\n",
        "\n",
        "    '''\n",
        " \n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', message)\n",
        "\n",
        "remove_emoji(\"RT @GlblCtzn: 'I don't wanna live forever – and nothing will because climate change' ����️�� @taylor...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y8hHtRQ5lPeh",
        "colab": {}
      },
      "source": [
        "# Apply function to remove emojis in Unicode\n",
        "train_data['message']=train_data['message'].apply(lambda x: remove_emoji(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X_s2WMsYlPel"
      },
      "source": [
        "### Remove Puncuations and Capital Letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mx_9piBGlPel",
        "colab": {}
      },
      "source": [
        "# Function to remove punctuation\n",
        "def remove_punct(text):\n",
        "    '''\n",
        "    Docstring here.\n",
        "\n",
        "    '''\n",
        "\n",
        "    table=str.maketrans('','',string.punctuation)\n",
        "    return text.translate(table)\n",
        "\n",
        "example=\"I am a #GREAT Man.,\"\n",
        "print(remove_punct(example))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-QXJ4Q6glPeo",
        "colab": {}
      },
      "source": [
        "# Apply function to remove punctuation\n",
        "train_data['message']=train_data['message'].apply(lambda x: remove_punct(x).lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hiz2av3clPer"
      },
      "source": [
        "### Remove numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wDihzNF5lPer",
        "colab": {}
      },
      "source": [
        "# Apply function to remove numbers from text\n",
        "train_data['message']=train_data['message'].apply(lambda x: re.sub('\\w*\\d\\w*', ' ', x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SyH_gBU0lPet"
      },
      "source": [
        "### Remove URL extra spaces as a result of cleaning text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s773hHM8lPet",
        "colab": {}
      },
      "source": [
        "# Apply function to remove extra spaces\n",
        "train_data['message']=train_data['message'].apply(lambda x: re.sub(' +',' ',x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dJ5ZfRyQxs8Y",
        "colab": {}
      },
      "source": [
        "# Apply function to remove undetected punctuations and unusual alphabets\n",
        "train_data['message']=train_data['message'].apply(lambda x: re.sub('[“”‘’—…ã¢â‚„¬â¦€]',' ',x))   #Vicky added this, removed all undetected punctuations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Df9s7v93lPev"
      },
      "source": [
        "### Check the spelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C0pN66mblPex",
        "colab": {}
      },
      "source": [
        "!pip install pyspellchecker\n",
        "from spellchecker import SpellChecker\n",
        "spell = SpellChecker()\n",
        "def correct_spellings(text):\n",
        "    '''\n",
        "    Function attempts to do spell checks but this proved to be a resource intensive task that did not run\n",
        "\n",
        "\n",
        "    Example: \n",
        "    \n",
        "    Input:\n",
        "\n",
        "\n",
        "    Output:\n",
        "\n",
        "\n",
        "    '''\n",
        "    corrected_text = []\n",
        "    misspelled_words = spell.unknown(text.split())\n",
        "    for word in text.split():\n",
        "        if word in misspelled_words:\n",
        "            corrected_text.append(spell.correction(word))\n",
        "        else:\n",
        "            corrected_text.append(word)\n",
        "    return \" \".join(corrected_text)\n",
        "        \n",
        "text = \"corect me plese\"\n",
        "correct_spellings(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dV02kjRxlPe2",
        "colab": {}
      },
      "source": [
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WAJNeZdsoWN-"
      },
      "source": [
        "### 5.2 Exploratory Data Analysis on Clean Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EtFOCFjNnsHC"
      },
      "source": [
        "### Clustering of Messages into Pro, News, Neutral and Anti Tweets\n",
        "\n",
        "The different Sentiment tweets are clustered based on the sentiments description to see what is common about the tweets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PKFPsjA1mua8",
        "colab": {}
      },
      "source": [
        "def getAnalysis(sentiment):\n",
        "    '''\n",
        "    Function to compute the news, pro, neutral,anti analysis\n",
        "\n",
        "    Example: \n",
        "    \n",
        "    Input:\n",
        "\n",
        "\n",
        "    Output:\n",
        "\n",
        "\n",
        "    '''\n",
        "    if sentiment == -1:\n",
        "        return 'Anti'\n",
        "    elif sentiment ==0:\n",
        "        return 'Neutral'\n",
        "    elif sentiment == 1:\n",
        "        return 'Pro'\n",
        "    else:\n",
        "        return 'News'\n",
        "\n",
        "train_data['Analysis']= train_data['sentiment'].apply(getAnalysis)\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HvQMTPY8nbG9",
        "colab": {}
      },
      "source": [
        "# All of the News Analysis Tweets\n",
        "news=1\n",
        "sorted_news= train_data.sort_values(by=['sentiment']) \n",
        "for i in range(0, sorted_news.shape[0]): \n",
        "    if sorted_news['Analysis'][i] == 'News':\n",
        "       print(str(news) + '.' + sorted_news['message'][i])\n",
        "       print()\n",
        "       news=news +1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z09etjIQzOiS",
        "colab": {}
      },
      "source": [
        "# All of the Pro Analysis Tweets\n",
        "pro=1\n",
        "sorted_pro= train_data.sort_values(by=['sentiment']) \n",
        "for i in range(0, sorted_pro.shape[0]): \n",
        "    if sorted_pro['Analysis'][i] == 'Pro':\n",
        "       print(str(pro) + '.' + sorted_pro['message'][i])\n",
        "       print()\n",
        "       pro=pro +1\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "smnEhCbH1PX2",
        "colab": {}
      },
      "source": [
        "# All of the Anti Analysis Tweets\n",
        "anti=1\n",
        "sorted_anti= train_data.sort_values(by=['sentiment']) \n",
        "for i in range(0, sorted_anti.shape[0]): \n",
        "    if sorted_anti['Analysis'][i] == 'Anti':\n",
        "       print(str(anti) + '.' + sorted_anti['message'][i])\n",
        "       print()\n",
        "       anti=anti +1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eyGW_91L1p-d",
        "colab": {}
      },
      "source": [
        "# All of the Neutral Analysis Tweets\n",
        "neu=1\n",
        "sorted_neu= train_data.sort_values(by=['sentiment']) \n",
        "for i in range(0, sorted_neu.shape[0]): \n",
        "    if sorted_neu['Analysis'][i] == 'Neutral':\n",
        "       print(str(neu) + '.' + sorted_neu['message'][i])\n",
        "       print()\n",
        "       neu=neu +1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L56Tb0IeAA2V"
      },
      "source": [
        "### Polarity\n",
        "\n",
        "Tells us how positive or negative a text is.\n",
        "\n",
        "It is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement and 0 is neutral.\n",
        "\n",
        "It simply means emotions expressed in a sentence.\n",
        "\n",
        "Emotions are closely related to sentiments. The strength of a sentiment or opinion is typically linked to the intensity of certain emotions, e.g., joy and anger."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wsOhP9SuEViB",
        "colab": {}
      },
      "source": [
        "!pip install textblob\n",
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kmpaxiEc_-6v",
        "colab": {}
      },
      "source": [
        "def GetPolarity(text):\n",
        "    '''\n",
        "    Function aims ro add polarity in the Train data\n",
        "\n",
        "    Example: \n",
        "    \n",
        "    Input:\n",
        "\n",
        "\n",
        "    Output:\n",
        "\n",
        "    '''\n",
        "    return TextBlob(text).sentiment.polarity \n",
        "\n",
        "train_data['Polarity']= train_data['message'].apply(GetPolarity)\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iRJ6SDtvHR4a"
      },
      "source": [
        "###Subjectivity\n",
        "\n",
        "Tells us how subjective or opinionated a text is.\n",
        "\n",
        "Sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qZP-tY_aH0ja",
        "colab": {}
      },
      "source": [
        "def GetSubjectivity(text):\n",
        "    '''\n",
        "    Funtion aims to add subjectivity on the Train data\n",
        "\n",
        "    Example: \n",
        "    \n",
        "    Input:\n",
        "\n",
        "\n",
        "    Output:\n",
        "\n",
        "\n",
        "    '''\n",
        "    return TextBlob(text).sentiment.subjectivity \n",
        "\n",
        "train_data['Subjectivity']= train_data['message'].apply(GetSubjectivity)\n",
        "\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BqclaJW9RBjm",
        "colab": {}
      },
      "source": [
        "#Plot the Subjectivity vs the Polarity\n",
        "plt.figure(figsize=(8,6))\n",
        "#for i in range(0, train_data.shape[0]):\n",
        "plt.scatter(train_data['Polarity'][:1000], train_data['Subjectivity'][:1000], color = 'purple')\n",
        "\n",
        "plt.title('Sentiment Analysis')\n",
        "plt.xlabel('Polarity')\n",
        "plt.ylabel('Subjectivity')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0DnsUZ863iI-"
      },
      "source": [
        "### WordCloud\n",
        "\n",
        "It is also known as a Text Cloud and whereby it is also a visualization where more specific word appear.\n",
        "\n",
        "It is a technique to show which words are the most frequent among the given text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AWnzkrna3ezb",
        "colab": {}
      },
      "source": [
        "#plot a word cloud\n",
        "from wordcloud import WordCloud\n",
        "Allwords= ' '.join( [tweets for tweets in train_data['message']] )\n",
        "wordCloud= WordCloud(width= 700, height= 500, random_state= 21, max_font_size= 150).generate(Allwords)\n",
        "plt.imshow(wordCloud, interpolation= 'bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "71C4f1-ehlvx",
        "colab": {}
      },
      "source": [
        "#Making use of word_tokenizer to tokenize the clean texts\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "train_data['Tokens']= train_data['message'].apply(lambda word: word_tokenize(word))\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "moBB3edxU9v5"
      },
      "source": [
        "### Bag of words\n",
        "\n",
        "'BOW' is a method to extract features from text documents. These features can be used for training machine learning algorithms. It creates a vocabulary of all the unique words occurring in all the documents in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FuCtbFgXdG7x",
        "colab": {}
      },
      "source": [
        "def bag_of_words_count(words, word_dict={}):\n",
        "    '''\n",
        "    Function takes in a list of words and returns a dictionary \n",
        "    with each word as a key, and the value represents the number of \n",
        "    times that word appeared\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    for word in words:\n",
        "        if word in word_dict.keys():\n",
        "            word_dict[word] += 1\n",
        "        else:\n",
        "            word_dict[word] = 1\n",
        "    return word_dict\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wlUcX1wyepL0",
        "colab": {}
      },
      "source": [
        "bag_words = {}\n",
        "for pp in train_data['sentiment']:\n",
        "    df = train_data.groupby('sentiment')\n",
        "    bag_words[pp] = {}\n",
        "    for row in train_data['Tokens'][:1000]:\n",
        "        bag_words[pp] = bag_of_words_count(row, bag_words[pp]) \n",
        "\n",
        "bag_words     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTFs4RC-ii6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "def count_words(input):\n",
        "    '''\n",
        "    Function returns the word count that exists only in Pro sentiment\n",
        "    \n",
        "    '''\n",
        "    cnt = collections.Counter()\n",
        "    for row in input:\n",
        "        for word in row:\n",
        "            cnt[word] += 1\n",
        "    return cnt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWALenUHvEs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data[(train_data.Analysis == 'Pro')][['Tokens']].apply(count_words)['Tokens'].most_common(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxxZO7RFyawF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data[(train_data.Analysis == 'Anti')][['Tokens']].apply(count_words)['Tokens'].most_common(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AWC9ndFuaugw"
      },
      "source": [
        "**Summary Feedback**\n",
        "\n",
        "On the WordCloud, words like Climate Change, Global Warming, Retweet, Protection Agency are the most frequent in the text. Bag of words shows the most frequent words and it corresponds to the wordcloud\n",
        "\n",
        "It is evident that if polarity is for example 0.8, which means that the statement is positive and 0.8 subjectivity, refers that mostly it is a public opinion and not a factual information.\n",
        "\n",
        " xxxxcan we analysis the scatter plot "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJAEFCskjThY",
        "colab_type": "text"
      },
      "source": [
        "###Testing Base Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HZL2CAOiLjU_",
        "colab": {}
      },
      "source": [
        "# import packages   \n",
        "\n",
        "# from comet_ml import Experiment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "# set plot style\n",
        "sns.set()\n",
        "import re\n",
        "import string\n",
        "import emoji\n",
        "import spacy\n",
        "import sklearn.feature_extraction.text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Importing wordcloud for plotting word clouds and textwrap for wrapping longer text\n",
        "from wordcloud import WordCloud\n",
        "from textwrap import wrap\n",
        "\n",
        "#from googletrans import Translator as translator\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn import metrics\n",
        "from nltk.corpus import stopwords\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8x3on-juLjVY",
        "colab": {}
      },
      "source": [
        "# load train data\n",
        "train_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KlJwaATOLjVh",
        "colab": {}
      },
      "source": [
        "# load test data\n",
        "test_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2KM5eBJQLjVr",
        "colab": {}
      },
      "source": [
        "# load sample data\n",
        "sample_data = pd.read_csv('https://raw.githubusercontent.com/Vicky-hub87/Team_ss1_Jhb-Classification-Predict/master/sample_submission.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MpAXbjKLMJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#combine train \n",
        "total = train_data.append(test_data)\n",
        "\n",
        "#change target to target1 in preparation for the word 'target' to be encoded as its own column\n",
        "total = total.rename(columns = {'sentiment':'sentiment1'})\n",
        "\n",
        "#create column for the number of words in tweet\n",
        "total['word count'] = total['message'].apply(lambda x: len(x.split()))\n",
        "\n",
        "total['character count'] = total['message'].apply(lambda x: len(x))\n",
        "\n",
        "#split so we can use updated train set with new feature\n",
        "train_data = total[:len(train_data)]\n",
        "\n",
        "  \n",
        "#add unique word count\n",
        "total['unique word count'] = total['message'].apply(lambda x: len(set(x.split())))\n",
        "\n",
        "\n",
        "#add url count\n",
        "total['url count'] = total['message'].apply(lambda x: len([i for i in x.lower().split() if 'http' in i or 'https' in i]))\n",
        "\n",
        "#add mention count\n",
        "total['mention count'] = total['message'].apply(lambda x: len([i for i in str(x) if i == '@']))\n",
        "\n",
        "#add hashtag count\n",
        "#total['hashtag count'] = total['text'].apply(lambda x: len([i for i in str(x) if i == '#']))\n",
        "\n",
        "#add punctuation count\n",
        "total['punctuation count'] = total['message'].apply(lambda x: len([i for i in str(x) if i in string.punctuation]))\n",
        "\n",
        "#split so we can use updated train set\n",
        "train_data = total[:len(train_data)]\n",
        "\n",
        "disaster = train_data['sentiment1'] == 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a37eVNbIMNBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remove punctuation\n",
        "def remove_punctuation(message):\n",
        "    return message.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "#remove stopwords\n",
        "StopWords = stopwords\n",
        "def remove_stopwords(message):\n",
        "    return ' '.join([i for i in message.split() if i not in StopWords])\n",
        "\n",
        "#remove words less than 4 \n",
        "def remove_less_than(message):\n",
        "    return ' '.join([i for i in message.split() if len(i) > 3])\n",
        "\n",
        "#remove words with non-alphabet characters\n",
        "def remove_non_alphabet(message):\n",
        "    return ' '.join([i for i in message.split() if i.isalpha() == True])\n",
        "\n",
        "#stem words\n",
        "stemmer = SnowballStemmer('english')\n",
        "def stem_words(message):\n",
        "    return stemmer.stem(message)\n",
        "\n",
        "#lemmatize words for verb\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_words_verb(message):\n",
        "    return lemmatizer.lemmatize(message, 'v')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgZAzFro9Wbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = train_data['message']  # this time we want to look at the text\n",
        "y = train_data['sentiment1']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz3votVRyah2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train) # need to make use of the original X_train set\n",
        "\n",
        "\n",
        "count_vectorizer=CountVectorizer(min_df=1, max_df=0.9, stop_words='english', decode_error='ignore')\n",
        "count_vectorized=count_vectorizer.fit_transform(X_train)\n",
        "\n",
        "import scipy.sparse\n",
        "\n",
        "X = scipy.sparse.hstack([X_train_tfidf, count_vectorized])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U8MCNdCj74X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_test_tfidf = vectorizer.fit_transform(X_test) # need to make use of the original X_test set\n",
        "\n",
        "\n",
        "count_vectorizer=CountVectorizer(min_df=1, max_df=0.9, stop_words='english', decode_error='ignore')\n",
        "count_vectorized=count_vectorizer.fit_transform(X_test)\n",
        "\n",
        "import scipy.sparse\n",
        "\n",
        "X = scipy.sparse.hstack([X_test_tfidf, count_vectorized])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odIAtvalsiTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oscLndTfkg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model classifier\n",
        "from sklearn.svm import LinearSVC\n",
        "clf = LinearSVC()\n",
        "clf.fit(X, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zbMLmY4XHp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.svm import LinearSVC\n",
        "\n",
        "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
        "                     ('clf', LinearSVC()),\n",
        "])\n",
        "\n",
        "# Feed the training data through the pipeline\n",
        "text_clf.fit(X_train, y_train) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gfO_O_b7dVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = text_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSu4vAqI4a-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.confusion_matrix(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu0zkWirG18j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(metrics.classification_report(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v7BU62EBpTF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(metrics.accuracy_score(y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYVlER3Gfkeo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model= LogisticRegression()\n",
        "\n",
        "model.fit(X,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONpNgVANU8kS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.svm import LinearSVC\n",
        "\n",
        "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
        "                     ('model', LogisticRegression()),\n",
        "])\n",
        "\n",
        "# Feed the training data through the pipeline\n",
        "text_clf.fit(X_train, y_train) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh5K00E_fkQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = text_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tnjDkwVQrGn7"
      },
      "source": [
        "# 6. Model Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BUd4WfSelPfF",
        "colab": {}
      },
      "source": [
        "def get_top_tweet_unigrams(corpus, n=None):\n",
        "\n",
        "    '''\n",
        "    Doctring HERE\n",
        "\n",
        "\n",
        "    '''\n",
        "\n",
        "\n",
        "    vec = CountVectorizer(ngram_range=(6, 6)).fit(corpus)\n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    return words_freq[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wRilqqxelPfZ",
        "colab": {}
      },
      "source": [
        "#Option 2: TFIDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf=TfidfVectorizer()\n",
        "text_tf = tf.fit_transform(train_data['message'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fLzOdxSnGM2u",
        "colab": {}
      },
      "source": [
        "#Option 2: TFIDF\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    text_tf, train_data['sentiment1'], test_size=0.67, random_state=42)\n",
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HJwCxCXHlPfd",
        "colab": {}
      },
      "source": [
        "#Option 3: TFIDF\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = train_data['message']  # this time we want to look at the text\n",
        "y = train_data['sentiment1']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ODPcEwVKlPfh",
        "colab": {}
      },
      "source": [
        "#Option 3: TFIDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b5gCDgQ6GM3Q"
      },
      "source": [
        "###### **Task: Train a Logistic Regression Classifier**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZlDKasiTObSl",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_model = LogisticRegression(C=1.0, solver='lbfgs', class_weight=None, multi_class='auto')\n",
        "lr_model.fit(X_train, y_train)\n",
        "pred_lr = lr_model.predict(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-nm10P1jEhx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iiWCv2galPfq"
      },
      "source": [
        "###### **Task: Train a Naive Bayes Regression classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tsvg8PdIlPfq",
        "colab": {}
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "\n",
        "# Model Generation Using Multinomial Naive Bayes\n",
        "clf = MultinomialNB().fit(X_train, y_train)\n",
        "pred_nb = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TMakwg7dlPfs"
      },
      "source": [
        "###### **Task: Train a Support Vector Machine (SVM) Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LQ8kXS9DlPfs",
        "colab": {}
      },
      "source": [
        "# Form a prediction set\n",
        "from sklearn.svm import LinearSVC\n",
        "clf = LinearSVC()\n",
        "clf.fit(X_train, y_train)\n",
        "pred_lsvc = clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n8Tss1onlPf0"
      },
      "source": [
        "### 6.1 Initial model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ExK08O3TlPf0"
      },
      "source": [
        "###### **Task: Test the Accuracy of the model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jFn1pR3BlPf0",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WkznqVNvlPf2",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o5BzTjNxlPf4",
        "colab": {}
      },
      "source": [
        "print(\"SVC Accuracy:\",metrics.accuracy_score(y_test, pred_lsvc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7x_rea8YlPf7",
        "colab": {}
      },
      "source": [
        "print(metrics.confusion_matrix(y_test,pred_lsvc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v9ddn5EvlPf9",
        "colab": {}
      },
      "source": [
        "print(metrics.classification_report(y_test,pred_lsvc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "73yS1r8cGM3X",
        "colab": {}
      },
      "source": [
        "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, pred_nb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ziEzuH9UlPgE",
        "colab": {}
      },
      "source": [
        "print(metrics.confusion_matrix(y_test,pred_nb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4TUQKFVDlPgG",
        "colab": {}
      },
      "source": [
        "print(metrics.classification_report(y_test,pred_nb))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DpUJd685lPgI",
        "colab": {}
      },
      "source": [
        "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, pred_lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jkBGO8ralPgK",
        "colab": {}
      },
      "source": [
        "print(metrics.confusion_matrix(y_test,pred_lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JeUC7M8HlPgN",
        "colab": {}
      },
      "source": [
        "labels = ['0: Neutral', '1: Pro', '2:News', '-1:Anti']\n",
        "\n",
        "pd.DataFrame(data=confusion_matrix(y_test, pred_lr), index=labels, columns=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8dniF8MklPgP",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# Saving each metric to add to a dictionary for logging to comet\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "precision = precision_score(y_test, pred_lr, labels=None, average='macro')\n",
        "recall = recall_score(y_test, pred_lr, labels=None, average='macro')\n",
        "f1 = f1_score(y_test, pred_lr, labels=None, average='macro')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bzy_34DVlPgQ",
        "colab": {}
      },
      "source": [
        "# Create dictionaries for the comet data we want to log\n",
        "\n",
        "#params = {\"random_state\": 7,\n",
        "          #\"model_type\": \"logreg\",\n",
        "          #\"scaler\": \"standard scaler\",\n",
        "          #\"param_grid\": str(param_grid),\n",
        "          #\"stratify\": True\n",
        "          #}\n",
        "\n",
        "metrics = {\"f1\": f1,\n",
        "           \"recall\": recall,\n",
        "           \"precision\": precision\n",
        "           }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KRVuytoGlPgS",
        "colab": {}
      },
      "source": [
        "# Log our comet parameters and results\n",
        "#experiment.log_parameters(params)\n",
        "#experiment.log_metric(\"accuracy\", f1)\n",
        "print(\"Logistic regression f1 macro score metrics: \", metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A5MywCwklPgT",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "print(metrics.classification_report(y_test,pred_lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jIlAselulPgV",
        "colab": {}
      },
      "source": [
        "experiment.end()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vD-jIGvElPgW"
      },
      "source": [
        "###### **Task: Display experiment on comet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vw8Yxc8ilPgX",
        "colab": {}
      },
      "source": [
        "experiment.display()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2W_ZOB_JlPgY"
      },
      "source": [
        "###### **Task: Save output of highest performing Model results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MFAOEYtzlPgY",
        "colab": {}
      },
      "source": [
        "sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "znPSm6qLlPga",
        "colab": {}
      },
      "source": [
        "clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S8SZoIgUlPgc",
        "colab": {}
      },
      "source": [
        "lr_model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T-QZhwYblPge",
        "colab": {}
      },
      "source": [
        "y_pred= pd.DataFrame(pred_lsvc).astype(int)\n",
        "base_df = pd.DataFrame()\n",
        "base_df['tweetid'] = test_data['tweetid']\n",
        "base_df['sentiment'] = y_pred\n",
        "sample_data['sentiment'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q73BQj-elPgf",
        "colab": {}
      },
      "source": [
        "#base_df.to_csv('/kaggle/working/submissionv4.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iOGU3Q2ilPgh",
        "colab": {}
      },
      "source": [
        "base_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SIR87MU-lPgu"
      },
      "source": [
        "###Summarise findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_sT1UmDGHQxq"
      },
      "source": [
        "# 7. Predictive Modelling\n",
        "\n",
        "Predictive modeling is a process that uses data and statistics to predict outcomes with data models. \n",
        "These models can be used to predict anything from sports outcomes and TV ratings to technological advances and corporate earnings. \n",
        "Predictive modeling is also often referred to as: Predictive analytics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_rkIlq-VFn1g"
      },
      "source": [
        "### 7.1 An Overview of learners\n",
        "**List of models we will train are as follows:**\n",
        "\n",
        "\n",
        "### Logistic Regression\n",
        "\n",
        "Logistic regression is used to obtain odds ratio in the presence of more than one explanatory variable (it explains the relationship between one dependent binary variable and one or more independent variables). \n",
        "\n",
        "Futhermore, Logstic regression is used to describe data. The procedure is quite similar to multiple linear regression, with the exception that the response variable is binomial(has a dependent variable with two possible values labeled 0 and 1). The result is the impact of each variable on the odds ratio of the observed event of interest. \n",
        "\n",
        "The main advantage is to avoid confounding effects by analyzing the association of all variables together. A disadvantage of this model may be overfitting whereby too many variables are added, which reduces the generalizability of the model beyond the data on which the model is fit. In this article, we explain the logistic regression procedure using examples to make it as simple as possible.\n",
        "(Sperandei, S., 2014. Understanding logistic regression analysis. Biochemia medica: Biochemia medica, 24(1), pp.12-18.)\n",
        "\n",
        "### K-Nearest Neighbours\n",
        "\n",
        "K-nearest neighbors (KNN) is a powerful, yet easy to understand machine learning algorithm. It relies on labeled input data to learn a function that produces an appropriate output when given new unlabeled data. In principle, this algorithm works by assigning the majority class of the N closest neighbors to the currect data point(it captures the idea of similarity).\n",
        "\n",
        "As such, absolutely no training is required for the algorithm! All we do is choose K (i.e. the number of neighbors to consider), choose a distance function to calculate proximity and we're good to go. As we decrease the value of K to 1, our predictions become less stable. \n",
        "\n",
        "Inversely, as we increase the value of K, our predictions become more stable making it more likely to make more accurate predictions to a certain point. where we will, eventually begin to witness an increasing number of errors. It is at this point we know we have pushed the value of K too far. The algorithm gets significantly slower as the number of examples and/or predictors/independent variables increase.\n",
        "(https://athena.explore-datascience.net/student/content/train-view/38/100/1783)\n",
        "\n",
        "### Support Vector Machines \n",
        "\n",
        "Support Vector Machines (SVMs) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. It separates data points using a hyperplane with the largest amount of margin. That's why an SVM classifier is also known as a discriminative classifier.\n",
        "\n",
        "The Support Vector Machine (SVM) algorithm is a popular machine learning tool that offers solutions for both classification and regression problems. Like logistic regression, they fit a linear decision boundary. \n",
        "\n",
        "However, unlike logistic regression, SVMs do this in a non-proabilistic way and are able to fit to non-linear data using an algorithm known as the kernel trick. In sklearn, these are called SVC (Support Vector Classifier) and SVR (Support Vector Regression) respectively. Classification of images can also be performed using SVM. (https://athena.explore-datascience.net/student/content/train-view/38/100/1783)\n",
        "\n",
        "### Naïve Bayes\n",
        "\n",
        "Naive Bayes is a classification algorithm that uses the principle of Bayes theorem to make classifications. The benefits of Naive Bayes are that the model is simple to build and is useful on large data sets. Further, it only requires a small number of training data to estimate the parameters necessary for classification.\n",
        "\n",
        "However the model makes an explicit assumption that the features are independent given the class label making it almost impossible to get a set of predictors which are completely independent. .(https://athena.explore-datascience.net/student/content/train-view/38/100/1783)\n",
        "\n",
        "### Decision Tree\n",
        "\n",
        "The Decision tree model uses a non parameterised approach to making predictions. Its basically a series of questions that the algorithm asks about a particular sample. For example, if the value is this then the target is that. The algorithm starts off by selecting a variable that produces the best split of the data. Every predictor is assigned an impurity scores based on how accurate its predictions are (measured by rmse). \n",
        "\n",
        "The feature with the lowest impurity score is used for the initial split at the top of the tree, know as the root node. The other variables are used for splits down the tree all the way to the final predictions at the leaf nodes. Trees can easily overfit the training data because of their slightly more complex nature compared with linear models. \n",
        "\n",
        "Therefore it is common to build a tree to maximum depth and then prune it by getting rid of branches that do not necessarily result in significantly lower rmse in the next node. Finally, tree can easily decipher non linear patterns in dataset and thusare often useful for non linearly distributed datasets. \n",
        "\n",
        "### Random Forest\n",
        "\n",
        "Random Forest is an extension of decision trees. Single decision tree often have high variance. Their predictions depend a lot on the data they were trained on. A slight change to the training set, their predictions change significantly. Because of this a popular technique is on of building multiple trees and averaging their predictions. This is known as bootstrap aggregation, meaning multiple models are trained on different subsets of the training data. The subsets are obtained through a bootstrap sampling approach where samples can be randomly selected more than once until the length of the bootstrap is equal to the length of the original training set. Note that some of the samples are left out (refered to as out of bag samples). The model is then trained on the bootstrap sample. Random forest takes sampling to yet another level by also using a subset of the predictors to build each tree. This results in not so highly correlated tree in the forest that if averaged can produce stable predictions with little variance. The user has the option to choose the number of tree of treat it as a hyper-paramer to be determined by the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g2i5ODUlPM59"
      },
      "source": [
        "### 7.2 An explanation of Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PcF1pIsHQKIr"
      },
      "source": [
        "A machine learning pipeline is used to help automate machine learning workflows.  They operate by enabling a sequence of data to be transformed and correlated together in a model that can be tested and evaluated to achieve an outcome, whether positive or negative.\n",
        "Machine learning pipelines are cyclical and iterative as every step is repeated to continuously improve the accuracy of the model and achieve a successful algorithm.  To build better machine learning models, and get the most value from them, accessible, scalable and durable storage solutions are imperative, paving the way for on-premises object storage.\n",
        "(Zhou, L., How to Build a Better Machine Learning Pipeline, 2018. URL https://www.datanami.com/2018/09/05/how-to-build-a-better-machine-learning-pipeline.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NQSGWZcAUETR"
      },
      "source": [
        "### 7.3 Build a pipeline to vectorize the data, then train and fit a model\n",
        "xxxx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "igV96lHaF2gh",
        "colab": {}
      },
      "source": [
        "#code here\n",
        "#from sklearn.pipeline import Pipeline\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from sklearn.svm import LinearSVC\n",
        "\n",
        "#text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
        "#                     ('clf', LinearSVC()),\n",
        "#])\n",
        "\n",
        "# Feed the training data through the pipeline\n",
        "#text_clf.fit(X_train, y_train)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q221LNzwYS2v"
      },
      "source": [
        "### 7.4 Run predictions and analyze the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9c8Yma6N0PNI",
        "colab": {}
      },
      "source": [
        "#code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "heading_collapsed": true,
        "id": "0OG-HJfcGM_w"
      },
      "source": [
        "# 8. Feature Selection and Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oFj1d73TdudP"
      },
      "source": [
        "Feature Selection is the process where you automatically or manually select those features which contribute most to your prediction variable or output in which you are interested in. Having irrelevant features in your data can decrease the accuracy of the models and make your model learn based on irrelevant features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uhvcJv0Mi1LT"
      },
      "source": [
        "The code cell below applies the function over multiple Variance Thresholds. A value between 0 and 50 has been chosen, increasing exponentially."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UugqmP-4h3sD"
      },
      "source": [
        "###### **Task: Perform Vector Arithmetic between features**\n",
        "xxxxx  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y6ez2svHh2f-",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qj26PJDt-mpV"
      },
      "source": [
        "###Summarise findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kC8CHFk0iviq"
      },
      "source": [
        "##### **Reguarisation - Improving model perfomance**\n",
        "xxxx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "24QgGCFIlPhR",
        "colab": {}
      },
      "source": [
        "`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J5Bkw1RVj0h9"
      },
      "source": [
        "###### Task: Implement L1 & L2 Regularisation for Logistic Regression\n",
        "A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. The key difference between these two is the penalty term. Ridge regression adds “squared magnitude” of coefficient as penalty term to the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4vR7XB3tjymJ",
        "colab": {}
      },
      "source": [
        "# code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XoYZOl2HzXLV"
      },
      "source": [
        "###### Task: Select the best variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aA6xggtEUJhd",
        "colab": {}
      },
      "source": [
        "# code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nYuvvuG5lPhf"
      },
      "source": [
        "### 8.1 Hyperparameter tuning\n",
        "\n",
        "In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm.\n",
        "These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7JvL_s0Gz1nC"
      },
      "source": [
        "###### Task: Adjust Hyperparameter/s of chosen model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ABXGqtLElPhg"
      },
      "source": [
        "xxxxx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E_9CCO6alPhg",
        "colab": {}
      },
      "source": [
        "# code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S0lS1mydlPhi"
      },
      "source": [
        "### Model Evaluation\n",
        "\n",
        "Model Evaluation is one of the most fundanental parts of the model development process. It helps to find the best model that represents the data at hand and how well the chosen model will work in the future.\n",
        "To avoid overfitting, both methods use a test set (not seen by the model) to evaluate model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gtbYpEIjlPhk"
      },
      "source": [
        "###### Task: Select the best model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g6A_MG7qslZP"
      },
      "source": [
        "xxxxxx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JEBgD4Xf2-_S",
        "colab": {}
      },
      "source": [
        "# code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JcsFJf6OGM__"
      },
      "source": [
        "# 9. Summary of Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PJn2DGXyt1C2"
      },
      "source": [
        "\n",
        "\n",
        "## Data Exploration\n",
        "xxxxx\n",
        "## Exploratory data analysis\n",
        "\n",
        "xxxxxx\n",
        "\n",
        "## Predictive modelling\n",
        "\n",
        "xxxxxx\n",
        "\n",
        "\n",
        "## Feature selection\n",
        "\n",
        "xxxxxx\n",
        "\n",
        "\n",
        "\n",
        "## Recommendations\n",
        "\n",
        "xxxxxx\n",
        "\n",
        "\n",
        "## Key takeaways\n",
        "xxxxxx\n",
        "\n",
        "\n",
        "\n",
        "## Referencing\n",
        "1.\n",
        "\n",
        "2.\n",
        "\n",
        "3. https://athena.explore-datascience.net/student/content/train-view/38/100/1783\n",
        "\n",
        "4. Sperandei, S., 2014. Understanding logistic regression analysis. Biochemia medica: Biochemia medica, 24(1), pp.12-18.\n",
        "\n",
        "5. Zhou, L., How to Build a Better Machine Learning Pipeline, 2018. URL https://www.datanami.com/2018/09/05/how-to-build-a-better-machine-learning-pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdfO0q_sfKMZ",
        "colab_type": "text"
      },
      "source": [
        "##"
      ]
    }
  ]
}